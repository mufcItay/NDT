---
title             : "Individual differences do not mask effects of unconscious processing"
shorttitle        : "Individual differences do not mask effects of UCP"

author: 
  - name          : "Itay Yaron"
    affiliation   : "1"
    corresponding : yes
    address       : "Haim Levanon 55, Tel Aviv, Israel"
    email         : "mufc.itay@gmail.com"
  - name          : "Nathan Faivre"
    affiliation   : "2"
  - name          : "Liad Mudrik"
    affiliation   : "1,3"
  - name          : "Matan Mazor"
    affiliation   : "4,5,6"

affiliation:
  - id            : "1"
    institution   : "Sagol School of Neuroscience, Tel Aviv University"
  - id            : "2"
    institution   : "University Grenoble Alpes, University Savoie Mont Blanc, CNRS, LPNC"
  - id            : "3"
    institution   : "School of Psychological Sciences, Tel Aviv University"
  - id            : "4"
    institution   : "Department of Psychological Sciences, Birkbeck, University of London"
  - id            : "5"
    institution   : "Wellcome Centre for Human Neuroimaging, UCL"
  - id            : "6"
    institution   : "All Souls College and Department of Experimental Psychology, University of Oxford"

authornote: |
  All data except for the datasets from Machado et al., 2007, 2009, for which participants did not consent to having their data shared online, and all analysis code are available at https://github.com/mufcItay/NDT. This study's analysis was not pre-registered, and all authors declare no conflict of interests.

abstract: |
  A wave of criticisms and replication failures is currently challenging claims about the scope of unconscious perception and cognition. Such failures to find unconscious processing effects at the population level may reflect the absence of individual-level effects, or alternatively, the averaging out of individual-level effects with opposing signs. Importantly, only the first suggests that consciousness may be necessary for the tested process to take place. To arbitrate between these two possibilities, we tested previously collected data where unconscious processing effects were not found (26 effects from 470 participants), using five frequentist and Bayesian tests that are robust to individual differences in effect signs. By and large, we found no reliable evidence for unconscious effects being masked by individual differences. In contrast, when we examined 136 non-significant effects from other domains, two novel non-parametric tests did reveal effects that were hidden by opposing individual results, though as we show, some of them might be driven by design-related factors. Taken together, five analysis approaches provide strong evidence for the restricted nature of unconscious processing effects not only across participants, but also across different trials within individuals. We provide analysis code and best-practice recommendations for testing for non-directional effects.

  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "unconscious processing; individual differences; consciousness"

bibliography      : "r-references.bib"

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - |
    \usepackage{setspace}
    \captionsetup[figure]{font={stretch=1,small}}

    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
classoption       : "man"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
output            : 
  papaja::apa6_word:
    fig_caption: true
urlcolor: blue
linkcolor: blue
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")

```

```{r initialize, warning = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

library(groundhog)
groundhog.library(c(
  'ggplot2',
  'stringr',
  'gridExtra',
  'ggh4x',
  'foreach',
  'ggtext',
  'ggridges',
  'ggpubr',
  'scales',
  'dplyr',
  'tidyr',
  'pracma',
  'extraDistr',
  'patchwork',
  'MCMCpack',
  'BayesFactor',
  'xfun'), '2023-07-22')
# from github (Commit: 9d338dd)
library(signcon)
  
source('datasets_analysis\\utils.R')
source('datasets_analysis\\definitions.R')

# extract the datasets results from the zipped results file
unzip('results\\results.zip', exdir = 'results\\')
```

```{r run-analyses, include=FALSE}
# run all analyses. The outputs are later used in the script
# to preprocess all datasets source the preprocessing script,
# sourcing this script will also download the up-to-date version of the confidence database from OSF (not the version used for analysis in this paper).
# source('datasets\\run_preprocessing.R')
# run all analysis on the preprocessed datasets
# source('datasets_analysis\\run_all_analyses.R')

# sourcing the 'fig1sim' script generates the panels populating Figure 1 in the manuscript, and analyzes the simulated data with all available non-directional (here we used an edited version of this panels)
# tests, and t-tests.
source('figures\\fig1sim.R')
# sourcing the 'fig2_3' script generates figure 2, the lower panel of figure 3, and Appendix C,G (note that figure 3 was edited to also include the illustration in the upper panel of figure 3)
# it also analyzes the results of all unconscious processing datasets according to all tests
source('figures\\fig2_3.R')
# sourcing the 'fig4' script generates figure 4 (minor edits were applied to the resulting svg figure).
# it also analyzes the results of all datasets (including datasets from non-UC domains) according to the sign consistency and absolute effect size tests.
source('figures\\fig4.R')

# we run the analysis of appendix A right before presenting the appendix (see below).

# appendix B is a table with a description of all unconscious processing studies, and it is created below in code

# appendix C is created by running source('figures\\fig2_3.R')

# the analysis and simulations for appendix D.
# to run the analysis again, uncomment the (commented) run_appendixD line and comment the read.csv line which reads the results of an existing simulations run.
source('appendix\\appendix_D.R')
# appendixD_res <- run_appendixD(apndxD_conf_FAs, apndxD_conf_sensitivity)
appendixD_res <- read.csv('appendix\\results\\2025-01-20_21_09_19Appendix_D_QUID_OANOVA.csv')

# the analysis and simulations for appendix E.
# to run the analysis again, uncomment the (commented) run_appendixE line and comment the read.csv line which reads the results of an existing simulations run (keeping the save_plot_appendixE line.)
source('appendix\\appendix_E.R')
# appendixE_res <- run_appendixE(apndx_e_conf)
appendixE_res <- read.csv('appendix\\results\\2025-01-26_07_53_42Appendix_E.csv')
save_plot_appendixE(appendixE_res)

# appendix F was not implemented in an external script.
# see the implementation below.

# appendix G is created by running source('figures\\fig2_3.R')

# the analysis and simulations for appendix H.
# to run the analysis again, uncomment the (commented) run_appendixH line and comment the read.csv line which reads the results of an existing simulations run.
source('appendix\\appendix_H.R')
apndxH_N_p <- N_p
apndxH_N_t <- N_t
# appendixH_res <- run_appendixH(apndx_e_conf)
appendixH_res <- read.csv('appendix\\results\\2025-01-26_17_09_45Appendix_H_ ABSES.csv')
apndx_H_median_cnd_power_abs_es <- appendixH_res %>%
  filter(N_p == median(appendixH_res$N_p), 
         N_t == median(appendixH_res$N_t),
         sds_ratio == median(appendixH_res$sds_ratio)) %>%
  pull(Power)

# the materials for appendix I are generated by sourcing the
# 'figure4.R' script, and the its figure was created with minor
# edits applied to the output files

```

```{r abstract-code, include=FALSE}
# we used fixed numbers on the abstract, here is the code to recreate them.

# get the number of participants and effects in the ucdb
ucdb_conf <- init_analysis(Unconscious_Processing_analysis_lbl)
ucdb_dfs <- get_input_df(ucdb_conf)
res_summary_fn <- 'Unconscious Processing_Results.csv'
results_fld <- 'results'
alpha <- .05
# read the ucdb results file
results <- read.csv(paste(results_fld, res_summary_fn, sep=.Platform$file.sep))
# filter the number of ns (non-significant) directional effects
ns_results <- results[results$directional_test.p > alpha,]
N_ns <- length(unique(ns_results$exp))
directional_effect_datasets <- setdiff(unique(results$exp), unique(ns_results$exp))
# count participants
ns_N_participants <- ucdb_dfs %>%
  group_by(exp) %>%
  filter(!exp %in% directional_effect_datasets) %>% 
  summarise(n = length(unique(idv)))
# removing experiments where the same participants were included in different
# effects
N_unique_participants <- sum(ns_N_participants %>% 
         filter(!exp %in% c('Faivre et al._1_incong', 'Faivre et al._3_incong', 
                            'Faivre et al._4_incong', 'Hurme et al_2020_TMS_Red', 
                            'Stein & van Peelen_2020_3_PAS_2', 'Zerweck et al_2021_3_Duration_20',
                            'Zerweck et al_2021_3_Duration_30', 'Zerweck et al_2021_2_39', 
                            'Zerweck et al_2021_2_29')) %>%
                            dplyr::pull(n))
```


# Introduction
Our brains simultaneously perform complex information processing functions and yet, at any given moment in time, only a small subset of these functions is accompanied by conscious experience. This raises the question: which brain functions depend on consciousness, and which functions can take place without it? 

One approach to investigating the scope and limits of unconscious processing is to measure the effect of different stimulus features on behaviour, while making sure that the stimulus itself is not consciously perceived (for review, see @kouider2007a; @reingold1988a). If a stimulus feature affects behaviour even when the participant is not aware of the stimulus, being conscious of the stimulus cannot be necessary for processing that feature.

For example, Dehaene and colleagues [-@dehaene1998a] studied the role of consciousness in semantic processing. In their seminal study, they presented a number word stimulus (henceforth, the prime), followed and preceded by strings of random letters, acting as backward and forward masks, rendering it invisible. Then, a fully visible number stimulus was presented (henceforth, the target; see Figure 1A). Participants were instructed to report whether the target stimulus was greater or smaller than the number five. Unconscious semantic priming was demonstrated by showing that participants responded faster in congruent trials, when the target and the prime were both smaller or larger than five (see again Figure 1A). This suggests that the numerical magnitude of the prime was processed unconsciously, affecting the response to the visible target number (see @damian2001a and @naccache2001a for critical assessment and discussions). In similar studies, participants were reported to unconsciously perform other high-level functions such as arithmetic operations [@sklar2012a], extract and integrate word meanings [@damian2001a; @gaal2014a; @sklar2012a], or scenes and objects [@mudrik2011a], detect errors [@charles2013a], and exert inhibition over responses [@gaal2008a] to stimuli that were masked from awareness. Findings of high-level processing in the absence of consciousness served to inform and reform theories of consciousness [@dehaene2001a; @oizumi2014a; @lamme2020a; @lau2011a].

However, more recent work has called into question some of these previous findings and their interpretations. First, many of the original results do not replicate when tested in independent samples of participants (using direct replications, e.g., @biderman2018a; @moors2019a; @stein2020a, or conceptual replications, e.g., @hesselmann2015a; @hesselmann2016a; @rabagliati2018a). Second, some of these findings might be driven by residual consciousness in a subset of trials due to unreliable awareness measures [@shanks2017a; @meyen2022a; @zerweck2021a; @moors2018a; @rothkirch2017a]. Indeed, when re-analyzed to properly control for this possibility, some of these effects disappear [@shanks2017a; @meyen2022a]. As a result, the scientific pendulum seems to be receding back to a narrower account of unconscious processing, consistent with a functional role of consciousness in most aspects of cognition [@balota1986a; @peters2017a; @moors2017a; @meyen2022a].

Overall, the field is still far from reaching a consensus regarding the scope and limits of unconscious processing. Although progress has been made in recent years toward improving methodology in unconscious processing studies, revealing the functional role of consciousness in cognition and perception remains difficult. Here we consider a largely neglected limitation of unconscious processing studies: by focusing on the average of signed (i.e., directional) single-participant summary statistics (for example, subtraction of reaction times between two conditions), previous investigations require not only that unconscious processing should leave a trace on behaviour, but also that this trace should be qualitatively similar across different participants (i.e., that the experimental manipulation would affect most participants in the same direction). We note that though this second requirement is intuitive, it is orthogonal with the theoretical question at stake; our main concern is whether a given stimulus feature can affect behaviour in the absence of consciousness, yet this does not necessarily imply that it affects all participants in the same way. This way, previous analyses of unconscious processing may have been too *conservative*, potentially missing effects that happen to vary between different participants (for a similar argument regarding cognitive science in general see @ince2022a). 

On the face of it, pronounced individual differences in unconscious processing effects on cognition and perception seem possible, even likely. Indeed, using objective measures of awareness such as discrimination ability (direct tasks; @schmidt2006a and @schmidt2024a, previous research revealed heterogeneity in participants’ susceptibility to different masking paradigms, and in the effects of design choices on participants’ ability to consciously perceive the masked stimulus. Participants have been shown to reliably vary in their susceptibility to the attentional blink [@martens2006a], and in the speed of breaking perceptual suppression— both in the breaking continuous flash suppression paradigm (b-CFS; [@sklar2021a] and the breaking repeated mask suppression one (b-RMS; [@abir2020a]. Furthermore, manipulating stimulus onset asynchrony (SOA, @albrecht2010a; see also @biafora2020a) and mask contrast (@biafora2020a) has been shown to produce different, sometimes opposite effects on metacontrast masking in different participants. Finally, visual imagery had different effects on the conscious perception of different participants in a binocular rivalry setting [@dijkstra2019a]. Some qualitative differences have been linked to variability in processing speed [@martens2006a], genetics [@maksimov2013a], and brain anatomy and physiology [@gaal2011a; @boy2010a].

Critically, unconscious processing effects (i.e. indirect tasks; @schmidt2006a) have also been shown to vary as a function of task parameters and across participants. At the heart of these findings are reports of “negative” priming effects: cases where the processing of the target was facilitated by an incongruent prime, rather than a congruent one. Indeed, masked priming effects changed in magnitude and even flipped in sign as a function of the interval between prime and target [@boy2014a; @boy2010b; @parkinson2014a; @schlaghecken2004a], or the presentation duration of stimuli rendered unconscious using either crowding [@faivre2011a] or CFS [@faivre2012a; @barbot2012a]. Negative effects were also observed for higher level features such as perceptual expectations: Bolger and colleagues [-@bolger2019a] showed that while most participants responded faster to upright faces in a b-CFS task, some responded faster to upside-down faces. Different hypotheses were laid out over the years regarding the driving mechanisms of the counter-intuitive negative priming effects. Among others, response inhibition of initial prime activations [@eimer2003a; @boy2010c], or neural habituation [@faivre2011a; @faivre2012a; @barbot2012a; @jacob2021a] were suggested. Taken together, it is not clear if, and to what extent, unconscious effects are subject to meaningful individual variability. Crucially, if they are, then some previously reported null results might actually be true effects, masked by such variability. 

The paper proceeds as follows. We first simulate a setting where a strong effect of unconscious processing on behaviour is entirely missed in standard analysis, due to pronounced inter-individual differences. We then show that the same effect is revealed when using three tests that are robust to population variability: the global null prevalence test [@donhauser2018a], Bayesian hierarchical modelling [@haaf2019a], and a test based on analysis of variance (ANOVA; [@miller2018a]. Importantly, unlike common measures of reliability which are used to directly estimate individual differences (see @parsons2019a), the above tests do not quantify individual differences, but measure group effects in a way that is robust to such differences. Hence, they provide researchers with the appropriate tools for detecting unconscious effects even if pronounced individual differences exist, without depending on that being the case. 

We then apply these tests to data gathered from eight unconscious processing studies (reporting 26 non-significant effects), and show that the same three tests support the null hypothesis according to which the behaviour of individual participants is unaffected by unconscious cognition and perception. This strengthens claims for a true absence of an effect in these studies. Finally, we propose two non-parametric alternatives that provide improved sensitivity and specificity, avoiding potentially unjustified statistical assumptions regarding the data-generating process. Our tests successfully reveal effects on multisensory integration, visual search, spatial attention and confidence ratings that could not be detected using standard directional analysis. However, similar to the three other approaches, our tests reveal no effects when applied to the studies of unconscious processing examined here. We conclude that existing data are most consistent with the absence of influences of unconscious stimuli on cognition and perception, not only at the population, but also at the single-participant level. 

# Simulating non-directional unconscious effects 

To provide a conceptual demonstration of how true causal effects of unconscious processing can be masked by inter-individual differences in effect signs, we simulated a typical experiment using a within-participants manipulation (Figure 1). Specifically, we generated trial-by-trial data from a standard unconscious priming experiment. For each simulated participant, we generated reaction time data from two conditions (corresponding to congruent and incongruent primes in unconscious processing studies). Individual-level effect sizes (in milliseconds) were sampled from a normal distribution centred at zero ($e_{i} \sim {\mathcal{N}}(0, \sigma_{b})$, where $e_{i}$ denotes the true effect size of the $i^{th}$ participant and $\sigma_b$ the between-participant standard deviation. Then, the trial-by-trial reaction times (RTs) of each participant and condition were generated according to each participant's true effect score ($e_{i}$), the relevant condition ($c \in \{1, 0\}$, where $c=1$ denotes the incongruent condition, and $c=0$ denotes the congruent condition), and the within-participant standard deviation ($\sigma_{w}$) ($RT_{i,c} \sim {\mathcal{N}} (0, \sigma_{w}) +c*e_{i}$).^[Similar results were obtained with more realistic Wald RT distributions as detailed in Appendix A.]

In two simulations, we manipulated two factors: the between-participant standard deviation (SD) over effect sizes ($\sigma_{b}$), and the within-participant SD over RTs within each condition ($\sigma_{w}$). This resulted in two distinct scenarios under this framework: (1) a *qualitative* or *non-directional differences* scenario, where the parameters of the generative model for all individuals were set to generate a true, non-zero effect (i.e., $e_i \neq 0$, for all individuals), but individual-level effects largely vary in magnitude and sign ($\sigma_{b}$=`r nde_sd_b`, $\sigma_{w}$=`r nde_sd_w`; Figure 1B), and (2) a *global null* scenario [@allefeld2016a; @nichols2005a], where no single participant is affected by the experimental manipulation ($\sigma_{b}$=`r sn_sd_b`, $\sigma_{w}$= `r sn_sd_w`; Figure 1C). We simulated $N_{t}$=`r sn_trials` trials per condition from $N_{p}$=`r sn_n` participants per scenario, noting that the general principle holds for other sample sizes and number of trials.

First, we analyzed this simulated data using a two-sided paired t-test on the differences in mean RTs between the two conditions. This is the standard protocol for testing if unconscious processing took place. In both simulations, we obtained a null result, revealing no evidence for a difference in RT between the congruent and incongruent conditions (*non-directional differences*: `r apa_print(nde_t_test)$full_result`; *global null*: `r apa_print(sn_t_test)$full_result`). Importantly, in the *non-directional differences* simulation, all participants were affected by the experimental manipulation (that is, their true effect sizes were different from zero). Thus, this commonly used approach systematically misses true causal effects of the experimental manipulation whenever they are inconsistent between participants.

To reiterate, a standard t-test misses existing individual-level effects because, operating on individual-level summary statistics, it cannot differentiate between within-participant variability in the dependent variable (noise) and meaningful between-participant variability. In recent years, researchers sought to address this limitation, advocating for the use of *non-directional* statistical methods that incorporate both within and between-participant variability. Here, we examined three non-directional approaches, all capable of detecting effects that exist within at least a single participant, without relying on group-level assumptions about the consistency of effect signs across individuals. As a result, a non-directional finding indicates that an effect exists within single participants without committing to whether the average effect at the group-level is positive, negative, or null.

First, the *prevalence global null* approach (@donhauser2018a; henceforth GNT) tests if the prevalence of individual-level effects in a given population (the proportion of individuals showing an effect) is greater than zero. The prevalence approach relies on a two-stage procedure. In the first stage, effects are tested at the individual level using a standard hypothesis-testing approach for a given significance level ($\alpha_{individual}$). In the second stage, the proportion of observed individual-level effects (the observed *prevalence*) is tested against $\alpha_{individual}$, which is the type-1 error rate of the individual-level test. This is done, using a one-sided binomial test (for which a separate significance level is used, here termed $\alpha_{prevalence}$, which may be different from $\alpha_{individual}$). Due to the one-sidedness of GNT, all confidence intervals on the prevalence of effects include 100% by definition (i.e., all individuals show an effect), and the crucial aspect is whether the lower bound of the confidence interval exceeds $\alpha_{individual}$. Hence, a significantly higher prevalence than $\alpha_{individual}$ means that the *global null* hypothesis, according to which no individual shows a true effect, can be rejected. 

Second, the *qualitative individual differences* approach (@rouder2021a; @haaf2019a; henceforth QUID) quantifies the relative support for the presence of “qualitative differences” in effects, that is, inter-individual differences in effect signs, by performing a Bayesian model comparison over a family of hierarchical models with different constraints [@haaf2019a]. Specifically, QUID models reaction times (RTs) using a standard linear model, where individual-level effects are treated as random effects. Then, to quantify the support for an effect while allowing for qualitative individual differences, it relies on Bayes Factors (BFs). The BF analysis compares evidence of a model that poses no constraints over the magnitude of individual-level effects, with a model in which all participants show no effect (see @rouder2021a for further model comparisons, aimed at examining different hypotheses regarding the character of population variability in effects). 

Third, Miller & Schwarz [-@miller2018a] introduce a parametric and frequentist test, based on ANOVA. Specifically, their Omnibus ANOVA test (henceforth OANOVA) probes the joint null hypothesis that there are no systematic differences neither between experimental conditions across individuals, nor within individuals and across trials. Together, this is equivalent to the *global null* scenario we presented above. OANOVA relies on a trial-level ANOVA model, comparing the variability in the dependent variable which is explained by the combination of the experimental manipulation and its interaction with the participants against the variability explained solely by participants' intercepts. Hence, OANOVA considers participants as fixed effects in the analysis, and enables detecting effects without assuming homogeneity in effect signs.

We applied the tests to our simulated data. For QUID, we used the default priors from the original publication [@rouder2021a]. For GNT and OANOVA, we used an $\alpha$ of `r oanova_alpha` to examine individual-level and group-level effects. For QUID, we considered $BF>`r bf_criteria`$ as evidence for an effect, $BF< \frac{1}{`r bf_criteria`}$ as evidence for no effect (*global null*), and values between these thresholds ($\frac{1}{`r bf_criteria`}\leq BF\leq `r bf_criteria`$) as inconclusive [@jeffreys1998a]. Reassuringly, all tests were able to differentiate between the two simulated scenarios, providing very strong evidence for an effect in the *non-directional differences* scenario, but not in the *global null* one. Specifically, the prevalence of effects on RT was clearly above the expected proportion of significant effects ($\alpha_{individual}$) in the *non-directional differences* simulation (using a two-sided t-test for the individual-level test; `r round(100 * nde_gnt_res$stat)`% significant effects, one-sided $CI_{95}$ = [`r round(100 * nde_gnt_res$ci_low)`, `r round(100 * nde_gnt_res$ci_high)`], p `r apa_p(nde_gnt_res$p)`), but this proportion was not higher than $\alpha_{individual}$ in the *global null* simulation (`r round(100 * sn_gnt_res$stat)`% significant effects, one-sided $CI_{95}$ = [`r round(100 * sn_gnt_res$ci_low)`, `r round(100 * sn_gnt_res$ci_high)`], p =`r apa_p(sn_gnt_res$p)`). Using the QUID method, a random effects model with individual-level effects was overwhelmingly preferred in the *non-directional differences* simulation ($BF$= `r formatC(nde_bf, format = 'e', 2)`), but a null model was preferred in the *global null* simulation ($BF$= `r round(sn_bf,2)`). Similarly, the OANOVA test revealed significant results in the *non-directional differences* scenario (F(`r nde_OANOVA_res$df_num`, `r nde_OANOVA_res$df_denom`) = `r round(nde_OANOVA_res$F,2)`, p `r paste0(ifelse(nde_OANOVA_res$p >= .001, '=', ''), apa_p(nde_OANOVA_res$p))`), and a non-significant effect in the *global null* simulation (F(`r sn_OANOVA_res$df_num`, `r sn_OANOVA_res$df_denom`) = `r round(sn_OANOVA_res$F,2)`, p `r paste0(ifelse(sn_OANOVA_res$p >= .001, '=', ''),apa_p(sn_OANOVA_res$p))`)^[Note that the unusually high degrees of freedom of OANOVA stem from modelling participants as fixed rather than random effects, as the global null model assumes no variability in effect sizes between participants.].

The simulations above demonstrate that adopting a non-directional approach, that is, an approach that takes into account the potential for opposite true effect signs among different participants, has the potential to reveal individual-level effects that would otherwise be missed due to high between-participant variability. Equipped with these validated tools, in the next section we use the QUID, GNT, and OANOVA tests to ask whether null results in the field of unconscious processing are driven by such inter-individual variability, or alternatively, whether they reflect the true absence of a causal effect.

**Figure 1**

*Simulated Unconscious Priming Data*

(ref:figure1caption) *Note*. Simulated data demonstrating how true effects of unconscious priming can be masked by heterogeneity at the population level. Panel A: stimuli in a typical unconscious processing experiment (based on Dehaene and colleagues, [-@dehaene1998a]). Participants make speeded decisions about a consciously perceived target stimulus (for example, its magnitude: being larger or smaller than the number five). The presentation of the target stimulus is preceded by a prime stimulus, which is masked from awareness. Decision time is measured as a function of prime-target agreement: congruent (blue) or incongruent (red). Panels B, C: Left: simulation parameters controlling the within ($\sigma_w$) and between ($\sigma_b$) participant SD. Right: the results that were generated using the simulation parameters. Each point depicts the measured individual-level summary statistics for the difference between the mean RTs of each condition (congruent and incongruent) with 95% confidence interval ($CI_{95}$) around each difference estimate, and the blue and red segments depict the $CI_{95}$ around the average of RTs in each condition separately (the grey segment in the middle of each CI) in the congruent and incongruent conditions, respectively. A constant of `r offset_rt`ms was added to the RTs in both panels for presentation purposes. Panel B: a *non-directional differences* scenario (simulated using the parameters $\sigma_{b}$=`r nde_sd_b`, $\sigma_{w}$=`r nde_sd_w`). Panel C: a *global null* scenario (no effect of the experimental manipulation; simulated using the parameters $\sigma_{b}$=`r sn_sd_b`, $\sigma_{w}$=`r sn_sd_w`). Since standard directional tests rely on individual-level summary statistics, they cannot arbitrate between the scenarios described in the two panels.
```{r figure1, out.width="100%", fig.cap = "(ref:figure1caption)"} 
knitr::include_graphics("figures//figure1.png")
```

# Reexamining unconscious effects
```{r empiric-all-tests, include=FALSE}
# get the counts of experiments for the section (uc datasets), reading from the enviroment created by soucing the fig2_3.R script
n_empiric_ns <- uc_ns_results %>% filter(is_sim == FALSE) %>% pull(exp) %>% unique() %>% length()
sc_sig_n <- uc_ns_results %>% filter(is_sim == FALSE, uc_ns_results$signcon.p < .05) %>% pull(exp) %>% unique() %>% length()
abs_es_sig_n <- uc_ns_results %>% filter(is_sim == FALSE, uc_ns_results$absolute_es.p < .05) %>% pull(exp) %>% unique() %>% length()
n_empiric_ns_quid <- ns_quid_res %>% filter(is_sim == FALSE) %>% pull(exp) %>% unique() %>% length()
n_empiric_ns_oanova <- ns_OANOVA_res %>% filter(is_sim == FALSE) %>% pull(exp) %>% unique() %>% length()
p_gnt_stat_zero <- ns_gnt_res %>% filter(is_sim == FALSE, gnt.stat == 0)%>% pull(exp) %>% unique() %>% length() / n_empiric_ns
p_null_QUID <- ns_quid_res %>% filter(is_sim == FALSE, quid_bf < 1/bf_criteria)%>% pull(exp) %>% unique() %>% length() / n_empiric_ns_quid
# get the results of individual experiments with significant sign consistency and absolute effect size.
# in the results data frame there are multiple lines per exp (listing the null distribution per experiment). We average across rows to get the repeating statistics per experiment.
res_per_exp <- uc_ns_results %>% group_by(exp) %>% summarise_all(mean)
skora_1_res <- res_per_exp %>% filter(exp == 'S1')
skora_2_res <- res_per_exp %>% filter(exp == 'S2')
bm_3_res <- res_per_exp %>% filter(exp == 'BM3')
# significant sign consistency correction:
skora_1_corrected_p_sc <- all_datasets_signcon$nsdir_res %>% filter(exp == "Skora et al_2020_1") %>% pull(test_result_corrected)
bm_3_corrected_p_sc <- all_datasets_signcon$nsdir_res %>% filter(exp == "Biderman & Mudrik_2018_3") %>% pull(test_result_corrected)
# significant absolute effect size correction:
skora_1_corrected_p_abs_es <- all_datasets_absES$nsdir_res %>% filter(exp == "Skora et al_2020_1") %>% pull(test_result_corrected)
skora_2_corrected_p_abs_es <- all_datasets_absES$nsdir_res %>% filter(exp == "Skora et al_2020_2") %>% pull(test_result_corrected)


# exatract the simulation results for sign-consistency and absolute effect size tests
nd_tests_sim_res <- uc_results %>% 
  filter(exp %in% c('GN', 'ND')) %>% 
  group_by(exp) %>% 
  dplyr::select(exp, signcon.statistic, absolute_es.statistic, signcon.p, absolute_es.p) %>%
  summarise_all(first)
sc_gn_stat <- nd_tests_sim_res[nd_tests_sim_res$exp=='GN',]$signcon.statistic
sc_gn_p <- nd_tests_sim_res[nd_tests_sim_res$exp=='GN',]$signcon.p
sc_nd_stat <- nd_tests_sim_res[nd_tests_sim_res$exp=='ND',]$signcon.statistic
sc_nd_p <- nd_tests_sim_res[nd_tests_sim_res$exp=='ND',]$signcon.p
abses_gn_stat <- nd_tests_sim_res[nd_tests_sim_res$exp=='GN',]$absolute_es.statistic
abses_gn_p <- nd_tests_sim_res[nd_tests_sim_res$exp=='GN',]$absolute_es.p
abses_nd_stat <- nd_tests_sim_res[nd_tests_sim_res$exp=='ND',]$absolute_es.statistic
abses_nd_p <- nd_tests_sim_res[nd_tests_sim_res$exp=='ND',]$absolute_es.p
```

To examine whether inter-individual differences masked true unconscious priming effects in previously reported studies, we collected and tested data from eight studies that reported null results [@biderman2018a; @faivre2014a; @stein2021a; @zerweck2021a; @benthien2021a; @hurme2020a; @skora2021a]; @chien2022a; all datasets and analysis scripts are publicly available online: [https://github.com/mufcItay/NDT](https://github.com/mufcItay/NDT)). 

We had three inclusion criteria: first, since all probed tests require trial-level data, only open-access datasets providing such data were included. Second, the independent variable of all studies had to be manipulated within single participants. And third, at least one non-significant effect was reported in the original study. Overall, this search strategy yielded data associated with `r n_empiric_ns` null effects (see Appendix B for details about all effects), 21 focusing on differences in RT and five on differences in signal detection sensitivity, d’ [@green1966a]. We attempted to include as many datasets as possible, combining literature search, open data repositories, and calls on social media. We used the criteria set by the original authors for demonstrating unawareness (e.g., using objective and/or subjective measures of awareness), and a two-sided non-parametric sign-flipping test on the population mean for filtering out experiments that showed significant directional effects^[Filtering out significant directional effects was done to validate previously reported null results, and to select the non-significant effects from studies that reported both significant and non-significant results. Across all RT effects, our analysis used raw RT scores, and thus our results diverged from the original results when log transformations were used (see the notes column in Appendix B for details).]. Finally, we excluded participants with fewer than five trials per experimental condition and/or zero variance in the dependent variable (e.g., when accuracy was measured). Together, these data allowed us to reexamine null unconscious processing effects using a non-directional approach that takes into account the potential for differences in effect signs when testing for group-level effects. We accordingly asked whether true effects of unconscious processing were masked by population heterogeneity in effect signs. Importantly, while we aimed for collecting as many datasets as possible, this is not a systematic review or meta-analysis. Accordingly, we conducted searches using Google Scholar, targeting any study investigating unconscious processing, while reporting at least one null result. Accordingly, it should be noted that our sample might miss relevant datasets due to the lack of systematic approach.

To that end, the effects of interest were tested using GNT, QUID, and the OANOVA tests (see Appendix C for an analysis of the significant directional effects which were excluded). GNT was applied to all `r n_empiric_ns` effects. In contrast, QUID and OANOVA were used on subsets of `r n_empiric_ns_quid` and `r n_empiric_ns_oanova` of these effects, respectively (omitting five effects of signal detection sensitivity, d’, from both tests, and one additional RT interaction from the QUID analysis, as its current implementation only supports simple RT effects).
All tests agreed on finding no reliable evidence for non-directional unconscious effects. According to GNT, the prevalence statistic was zero in `r round(100*p_gnt_stat_zero)`% of the effects (maximal observed prevalence = `r ns_gnt_res %>% filter(is_sim == FALSE) %>% mutate(prev=round(100*gnt.stat)) %>% pull(prev) %>% max()`%; see Fig. 2A), and the 95% one-sided CI included $\alpha=5$% in all of them. Hence, for all effects the prevalence of effects did not exceed the expected rate under the global null hypothesis. Similarly, for both QUID and the OANOVA tests, no single $BF$ or p-value revealed evidence for an effect (maximal $BF_{10}$ = `r round(ns_quid_res %>% filter(is_sim == FALSE) %>% pull(quid_bf) %>% max(), 2)` and all p-values > `r oanova_alpha`; see Figure 2B, C). Notably, QUID obtained moderate evidence for the *global null* model in `r round(100*p_null_QUID,2)`% of the cases (see Fig. 2B). The remaining effects were inconclusive. Hence, for the effects collected here, in the case of unconscious processing, the three tests revealed a highly similar pattern of results, consistent with a strong interpretation of previously reported null results as revealing the genuine absence of a causal effect of unconsciously perceived stimuli on behaviour.

**Figure 2**

*Reanalysis of Unconscious Processing Effects*

(ref:figure2caption) *Note*. The results of applying the GNT (A), QUID (B), and OANOVA (C) tests to effects that produced null results in a non-parametric directional test and to simulated data (the Non-directional effect (ND) and Global Null (GN) simulations described above, presented as square-shaped markers). Effect labels^[Effect labels abbreviations (sorted alphabetically): BH = @benthien2021a, BM = @biderman2018a, C = @chien2022a, F = @faivre2014a, H = @hurme2020a, S = @skora2021a, SV = @stein2021a, Z = @zerweck2021a. For all labels, numbers denote effect indices within each study (see Appendix B for the full mapping between labels and effects).] appear on the x-axis. Panel A: the estimated prevalence of an unconscious effect in each of the cases, using GNT [@donhauser2018a]. Segments depict the one-sided 95% CI ($CI_{95}$) for the prevalence estimate, hence for GNT all CIs include 100% by definition, and the crucial test for an effect is whether the lower bound of each CI includes the $\alpha$ level used to test for individual-level effects ($\alpha_{individual}$). The solid orange line indicates the expected prevalence of 5% significant individual-level effects, given that individual effects were tested using $\alpha_{individual}=0.05$ (type-1 error rate). Panel B: Bayes factors for the comparison between a random effects model that takes into account potential differences in effect signs and the global null model. White markers depict cases where moderate evidence for the global null model was found, while grey markers indicate inconclusive results. The dashed black line indicates a BF of 1 (no preference for either model), and the solid orange lines indicate a BF cutoff of `r bf_criteria`. Panel C: p-values obtained by the OANOVA test [@miller2018a]. Blue and grey markers indicate significant and non-significant results, respectively. For illustration purposes, BF and significance values are presented on a logarithmic scale on the y-axis.
```{r figure2, out.width="100%", fig.cap = "(ref:figure2caption)"}
knitr::include_graphics("figures//plots//ns_available_methods_res.png")
```

Yet, the reviewed approaches also have some limitations that make it harder to draw firm conclusions based on their results. First, in contrast to frequentist tests within the Null Hypothesis Statistical Testing (NHST) tradition, QUID is designed to quantify relative evidence, and as such provides only weak^[Specifically, p($BF_{10}>x|H_0$) is never higher than $\frac{1}{x})$. Proof: assume that $BF_{10} > 3$ for observed values $v \in V$. That is, for every $v \in V$, $p(v|H_1)>3p(v|H_0)$. The probability of making a type-1 error $\alpha$ equals $p(v \in V|H_0)$. It follows that $p(v \in V|H_1)>3\alpha$. Since this probability cannot be higher than 1, $\alpha$ cannot exceed $\frac{1}{3}$ (for a similar derivation see the universal bound in @royall2000a] control over long-term error rates (the probability of finding a false positive result or missing a true result over an infinite number of tests, with the former being more critical to our point here). Such error control promises a much-needed ‘fool-proof’ method to infer the existence of unconscious processing effects without making too many mistakes in the long run [@lakens2020a; @kelter2021a].

Second, both the model comparison approach used in QUID and the OANOVA test necessarily assume a parametric model of the data, making specific assumptions of normality and equal within-individual variance. In simulations, we find that violations of this second assumption can have dramatic effects on the specificity and sensitivity of both tests (see Appendix D). This can be addressed by more complex models that are capable of handling different distribution families, but as model complexity grows, unwanted effects of assumption violations may become harder to spot and quantify. Hence, taking a non-parametric approach provides safer inferences when the form of the data-generating process is not fully known. 

Lastly, since GNT is focused on the prevalence of effects, it begins with testing the significance of effects at the single subject level, thereby dichotomizing a continuous test statistic into one bit of information: significant or not. This dichotomization results in information loss and introduces an additional free parameter — the individual alpha level. This step is well justified when estimating population prevalence, but it is unnecessary for our purpose of detecting a non-directional effect at the population level. As we describe below, using a continuous participant-level statistic makes our test more sensitive (see Appendix E for a direct comparison between the two
approaches).

In the next section, we introduce two novel non-directional tests that take into account population heterogeneity to infer group-level effects. The tests are both frequentist and non-parametric, which addresses the above issues. Similarly to the OANOVA test, they promise a tight control for long term error-rates, but unlike it, our tests do not assume a parametric model of the data-generating process. Using a continuous within-participant summary statistic, they are also more statistically powerful than approaches that focus on a dichotomous notion of effect prevalence (see Appendix E). 


# Two non-parametric tests that are robust to qualitative differences 

We propose two tests that follow these two principles: a within-participant effect is convincing if it is consistently evident across different trials, and if its magnitude, in standardised units, is larger than expected by chance alone. 


First, the *Sign Consistency* test examines the consistency of effects within participants by estimating the probability that splitting the trials of an individual into two random halves would result in both halves showing the same qualitative effect (e.g., for both halves, the performance in the congruent condition is higher than in the incongruent condition; see Fig. 3A). By doing this many times (in our implementation, 500 times), we can measure how often the two halves agree. Following this strategy, we estimate the consistency of effect signs within each individual by measuring the frequency of consistent results across splits. Then, we compare the group-mean consistency score against a null distribution: 10,000 samples of group-level consistency scores, obtained after randomly shuffling the experimental condition labels within participants, effectively breaking any correlation between conditions and the dependent variable (here, to speed up the computational process, for each participant, 100 permutations were created, from which we randomly sampled a single permutation in each null distribution sample; @stelzer2013a).Hence, our null distribution reflects the expected consistency of within-participant effects when the dependent variable of no single participant is sensitive to the experimental manipulation (since by shuffling the labels of the independent variable within participants we nullified its potential effect on the dependent variable). 

As an alternative test, we propose the *Absolute Effect Size* test^[We would like to thank Sascha Meyen, a Reviewer of this paper, for the extremely helpful suggestion to implement and use the absolute effect size test.], which examines whether the absolute value of within-participant effect sizes exceeds the absolute value of such effects obtained solely due to noise (for a similar measure of consistency see the modulation index in @buaron2020a). To that end, we estimate the standardized effect size of each participant (here using Cohen's d for RT effects, and d’ for accuracy effects), and compute the group average. We then compare the group average of absolute effect sizes with a label-shuffled null distribution, using the same procedure to construct the null distribution as we do for the sign-consistency test above.

An easy-to-use implementation of both tests is available as part of the signcon R package ([https://github.com/mufcItay/signcon](https://github.com/mufcItay/signcon); see Appendix F for extensions of the sign-consistency test to use cases that diverge from simple mean difference between conditions. Similar extensions can be implemented for the absolute effect size test).

**Figure 3**

*The Proposed Tests*

(ref:figure3caption) *Note*. Two frequentist, non-parametric, tests for non-directional effects. Panel A,B: schematic illustrations of the sign consistency test (A) and absolute effect size (B) tests, using the same conventions as in Figure 1 (C = congruent, I = incongruent). In Panel A, participant-wise sign consistency is quantified as the proportion of random splits of experimental trials, for which both halves display the same qualitative effect (C>I or I>C). The upper row illustrates the overall RT data for one participant, and each row below shows one split of the data. For each half we compare the mean of congruent and incongruent RT distributions, to test if the direction of the difference in the two halves is consistent or not. The averaged consistency score across participants (plotted in green) is then compared to the non-parametric null (plotted to the left of the framed box), to obtain a significance value. Panel B similarly depicts the scheme for quantifying participant-wise absolute effect size. Significance value is obtained in the same way described for the sign consistency test. In this hypothetical case, the group does not show an effect according to both tests, as the average score is well within the null distribution. Both tests can be used with other measures, such as d’, correlations, and indexes of metacognitive sensitivity. Panel C,D: the results of applying the tests to effects that produced null results in a non-parametric directional test (N = `r n_empiric_ns`). Panel C: the results obtained by the absolute effect size test for the same datasets (N = `r n_empiric_ns`). Significant results, for which the estimated group-level mean sign consistency (C) or absolute effect size (D) statistic is greater than 95% of the null distribution, are marked in blue. As in Figure 2, the x-axis lists effect labels.
```{r figure3, out.width="100%", fig.cap = "(ref:figure3caption)"}
knitr::include_graphics("figures//figure3.png")
```

We applied both tests to the same effects examined in Fig.2 (all studies for which a directional test did not produce significant results; see Appendix G for an analysis of the excluded significant directional effects). The results revealed a similar picture to the one provided by the previous analyses (see Figure 3B). First, for the simulated datasets, the sign consistency test obtained non-significant results in the *global null* scenario (SC = `r round(100*sc_gn_stat)`%, p = `r apa_p(sc_gn_p)`), and detected an effect in *non-directional differences* scenario (SC = `r round(100*sc_nd_stat)`%, p `r apa_p(sc_nd_p)`). Similar results were obtained for the absolute effect size test (|ES| = `r round(abses_gn_stat,2)`, p = `r apa_p(abses_gn_p)` and |ES| = `r round(abses_nd_stat,2)` ,p `r apa_p(abses_nd_p)`, respectively). Second, for the empirical datasets, the vast majority of cases did not show significant effects, with three exceptions. In two cases, the absolute effect size test revealed a non-directional effect of an unconsciously presented cue on wagering decisions (S1: |ES| = `r round(skora_1_res %>% pull(absolute_es.statistic) %>% first(),2)`, p = `r apa_p(skora_1_res %>% pull(absolute_es.p) %>% first())` and S2: |ES| = `r round(skora_2_res %>% pull(absolute_es.statistic) %>% first(),2)`, p = `r apa_p(skora_2_res %>% pull(absolute_es.p) %>% first())`). Only the first of the two showed a non-directional effect according to the sign consistency test (S1; SC = `r round(100*skora_1_res %>% pull(signcon.statistic) %>% first())`%, p = `r apa_p(skora_1_res %>% pull(signcon.p) %>% first())`). In a third case, the sign-consistency test, but not the absolute effect size test, revealed a scene-object congruency effect (Biderman and Mudrik, 2018; SC = `r round(100*bm_3_res %>% pull(signcon.statistic) %>% first())`%, p = `r apa_p(bm_3_res %>% pull(signcon.p) %>% first())`). Although these effects were not detected by GNT, the prevalence of observed proportion of individual-level effects was above zero for all three (`r round(100*skora_1_res %>% pull(gnt.stat) %>% first())`%, `r round(100*skora_2_res %>% pull(gnt.stat) %>% first())`% and `r round(100*bm_3_res %>% pull(gnt.stat) %>% first())`%, respectively). Thus, despite some evidence for non-directional effects, the overall picture remained the same, hinting at minimal qualitative inter-individual differences in unconscious processing.

```{r nd_empiric_power_tests, include=FALSE}
individual_effect_alpha <- .05
get_expected_prevalence <- function(prevalence, power, alpha) {
  return(prevalence * power + (1-prevalence) * alpha)
}
# 
full_prevalence_exp_rate <- get_expected_prevalence(1, apndx_H_median_cnd_power_abs_es, individual_effect_alpha) 
low_prev_ratio <- 1/8
low_prevalence_exp_rate <- get_expected_prevalence(low_prev_ratio, apndx_H_median_cnd_power_abs_es, individual_effect_alpha) 
third_prev_ratio <- 1/3
third_prevalence_exp_rate <- get_expected_prevalence(third_prev_ratio, apndx_H_median_cnd_power_abs_es, individual_effect_alpha) 

# decreasing the two Skora effects from the overall number
n_unconscious_effects_abs_es <- abs_es_sig_n - 2
n_tested_effects_abs_es <- n_empiric_ns - 2
test_full_prevalence <- binom.test(n_unconscious_effects_abs_es, n_tested_effects_abs_es, full_prevalence_exp_rate, alternative = 'less')
test_low_prevalence <- binom.test(n_unconscious_effects_abs_es, n_tested_effects_abs_es, low_prevalence_exp_rate, alternative = 'less')
test_third_prevalence_all_datasets <- binom.test(abs_es_sig_n, n_empiric_ns, third_prevalence_exp_rate, alternative = 'less')
```
Together, five different analysis methods support the conclusion that by and large, unconscious priming effects are not masked by individual differences. Yet one can still claim that these statistical tests are simply not sensitive enough to detect qualitatively variable, non-directional effects, even when those exist. To test this claim, we conducted two additional analyses: First, we used simulations to estimate the sensitivity of our solutions to non-directional effects with various effect sizes, determined according to previous analyses on unconscious processing [@meyen2022a] and cognitive control [@rouder2023a]. The results corroborated the concerns for lack of power when using common unconscious processing settings of the number of participants and trials (see @baker2021 for a detailed analysis of the contribution of both factors to power). Yet, we conducted further analysis showing that given the (low) power estimates we found, the number of significant effects obtained by the absolute effect size test would be surprisingly low if an unconscious effect existed in all or even one-eighth of the datasets (see Appendix H). Second, to provide positive-control for these methods and show that they can be used to reveal such hidden effects in other fields, we collected additional, openly accessible, datasets from studies conducted in different fields of research within experimental psychology. We then used our non-parametric tests on these datasets, demonstrating its potential benefit in determining whether a null result at the group level hides true, but variable, effects at the individual participant level.

# Positive control: Testing within-participant non-directional effects across experimental psychology studies
```{r other-domains-tests, include=FALSE}
# get the counts of experiments for the section (from all domains), reading from the environment created by sourcing the fig4.R script
N_ns_no_UC <- sum(all_datasets_signcon$nsdir_N %>% 
                    filter(type != "Unconscious Processing") %>% 
                    dplyr::pull(N))
N_ns_Confidence <- sum(all_datasets_signcon$nsdir_N %>% 
                         filter(type %in% c('Confidence', 'Metacognitive Sensitivity')) %>%
                         dplyr::pull(N))
N_ns_Metacog <- all_datasets_signcon$nsdir_N %>% filter(type == 'Metacognitive Sensitivity') %>% dplyr::pull(N)
N_ns_psycog <- all_datasets_signcon$nsdir_res %>%
  mutate(str_type = levels(all_datasets_signcon$nsdir_res$type)[type]) %>%
  filter(startsWith(str_type, 'Cognitive')) %>%
  mutate(sub_cat =  ifelse(startsWith(exp,'Adam'), 'VS', 
                           ifelse(startsWith(exp,'Battich'), 'Social' ,
                                  'Reproducibility'))) %>%
  group_by(sub_cat) %>% 
  summarise(n = n())
N_ns_Reproducibility <- N_ns_psycog %>% filter(sub_cat == 'Reproducibility') %>% pull(n)
N_ns_Attention <- N_ns_psycog %>% filter(sub_cat == 'VS') %>% pull(n)
N_ns_Social <- N_ns_psycog %>% filter(sub_cat == 'Social') %>% pull(n)

# extract individual experiments results
alpha <- .05
battich_res <- all_datasets_signcon$nsdir_res %>% filter(startsWith(exp, 'Battich'))
battich_res_sig <- battich_res %>%
  filter(test_result <= alpha)
all_ns_adam_res <- all_datasets_signcon$nsdir_res %>% 
  filter(startsWith(exp,'Adam'), directional_test.p > alpha)
ns_adam_res <- all_datasets_signcon$nsdir_res %>% 
  filter(startsWith(exp,'Adam'), directional_test.p > alpha, test_result <= alpha)
ns_adam_res_sig <- ns_adam_res %>%
  filter(test_result <= alpha) %>%
  nrow()

# significant absolute effect size for reproducibility project
Forti_res <- all_datasets_absES$nsdir_res %>% filter(type == "Cognitive Psychology", startsWith(exp, 'Fort'))
Estes_res <- all_datasets_absES$nsdir_res %>% filter(type == "Cognitive Psychology", startsWith(exp, 'Este'))
# for the multisensory experiment (Battich et al.), we also examine whether the results can be explained by a directional order effect
# read the whole dataset
battich_data <- read.csv("datasets\\cogdb\\Battich_etal_2021.csv")
# test for difference in effects between halves (we included only the individual and joint attention condition, so halves = blocks)
compare_halves <- function(data, exp_label) {
  # get the directional effect for each of the halves
  res_1st <- get_directional_effect(data %>% 
                                       filter(exp == exp_label, half == 1),
                                     idv = 'idv', iv = 'iv2', dv ='dv')
  scores_1st = res_1st$effect_per_id$score
  res_2nd <- get_directional_effect(data %>% 
                                       filter(exp == exp_label, half == 2),
                                     idv = 'idv', iv = 'iv2', dv ='dv')
  scores_2nd = res_2nd$effect_per_id$score
  # use a t-test to compare effect scores between halves (is there a condition-order effect?)
  return(t.test(scores_1st, scores_2nd,paired = TRUE))
}
# test for a directional condition-order effect in all of the cases where a non-directional effect was found (three cases our of four) 
res_battich_resp1_t <- compare_halves(battich_data[battich_data$exp == battich_res$exp[1],], battich_res$exp[1])
res_battich_resp2_t <- compare_halves(battich_data[battich_data$exp == battich_res$exp[2],], battich_res$exp[2])
res_battich_rt2_t <- compare_halves(battich_data[battich_data$exp == battich_res$exp[3],], battich_res$exp[3])

# get effects for which non-directional effects were already reported
machado_etal_fld <- 'datasets\\additional_analyses\\Machado_etal_2007_2009'
# the data was shared with us privately, so here we return the obtained statistics if data is not available
source(paste(machado_etal_fld, "process_Machado_2007_2009.R", sep = .Platform$file.sep))
machado_2007_res <- analyze_machado(2007, machado_etal_fld)
machado_2009_res <- analyze_machado(2009, machado_etal_fld)
machado_2007_350 <- machado_2007_res[machado_2007_res$SOA == 350,]
machado_2009_650 <- machado_2009_res[machado_2009_res$SOA == 650,]
```

We used the proposed tests to expose hidden effects that were not revealed by standard directional tests in various fields of research (see Appendix I for the same analysis using the three other tests). To that end, we exhausted all data from different open-access databases (the Confidence Database [@rahnev2020a], the Reproducibility Project [@collaboration2015a], and the Classic Visual Search Effects open dataset [@adam2021a]). We used the same inclusion criteria from the unconscious processing studies analysis, detailed above. Again, effects that were significant according to a non-parametric, directional sign-flipping test on the population mean were filtered out. Overall, we collected data associated with `r N_ns_no_UC` non-significant effects (`r N_ns_Confidence` from the Confidence Database, `r apa_num(as.integer(N_ns_Reproducibility), numerals = FALSE)` from the Reproducibility Project, `r apa_num(as.integer(N_ns_Attention), numerals = FALSE)` from the Classic Visual Search Effects open dataset and `r apa_num(as.integer(N_ns_Social),numerals = FALSE)` from the social media query). In all cases, participants were excluded for having fewer than five trials per experimental condition and/or zero variance in the dependent variable. 

We grouped the different effects into three categories, according to research topics and the analysis we used to test them: first, we tested for effects of participants’ responses in 2-alternative forced choice tasks on their confidence ratings in all datasets from the Confidence Database (@rahnev2020a; retrieved on 23/1/2023), by comparing the mean confidence ratings between two different responses. Second, we used the same Confidence Database datasets to test for metacognitive sensitivity effects of response. Metacognitive sensitivity, that is, the agreement between objective accuracy and subjective confidence, was quantified as the area under the response-conditional type-2 Receiver Operating Characteristic curve (@meuwese2014a; here we also excluded datasets that did not include accuracy scores; the remaining `r N_ns_Metacog` effects were analyzed). Third, we grouped effects from the Reproducibility Project [@collaboration2015a], the Classic Visual Search Effects open dataset [@adam2021a], and a single study from the social media query [@battich2021a] under a more general “Cognitive Psychology” category. For these studies, we tested the sign consistency and significance of absolute effect size for the effect tested by the original authors (averaged difference or interaction effects).

Across the entire sample, including all analyzed effects (N = `r N_ns_no_UC`), most effects showed significant sign consistency (`r  round(all_datasets_signcon$sig_percent_nsdir_across_types)`%, N=`r all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type != "Unconscious Processing") %>% dplyr::pull(N) %>% sum()`) and absolute effect size effects (`r  round(all_datasets_absES$sig_percent_nsdir_across_types)`%, N = `r all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type != "Unconscious Processing") %>% dplyr::pull(N) %>% sum()`). This trend was further explored within each category. Out of `r all_datasets_signcon$nsdir_N %>% filter(type =="Confidence") %>% pull(N)` null confidence effects, `r all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type =="Confidence") %>% dplyr::pull(N)` (`r round(all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type =="Confidence") %>% dplyr::pull(perc_nondir_sig),0)`%) were revealed to be non-directional effects according to the sign consistency test, and `r all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Confidence") %>% dplyr::pull(N)` (`r round(all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Confidence") %>% dplyr::pull(perc_nondir_sig),0)`%) according to the absolute effect size test. Out of `r all_datasets_signcon$nsdir_N %>% filter(type =="Metacognitive Sensitivity") %>% pull(N)` null metacognitive sensitivity effects, the two tests revealed `r all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type =="Metacognitive Sensitivity") %>% dplyr::pull(N)` (`r round(all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type =="Metacognitive Sensitivity") %>% dplyr::pull(perc_nondir_sig),0)`%) and `r all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Metacognitive Sensitivity") %>% dplyr::pull(N)` (`r round(all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Metacognitive Sensitivity") %>% dplyr::pull(perc_nondir_sig),0)`%) significant non-directional effects, respectively. Finally, out of `r all_datasets_signcon$nsdir_N %>% filter(type =="Cognitive Psychology") %>% pull(N)` null effects in the cognitive psychology category, `r all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type =="Cognitive Psychology") %>% dplyr::pull(N)` (`r round(all_datasets_signcon$sig_percent_nsdir_per_type %>% filter(type =="Cognitive Psychology") %>% dplyr::pull(perc_nondir_sig),0)`%) and `r all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Cognitive Psychology") %>% dplyr::pull(N)` (`r round(all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Cognitive Psychology") %>% dplyr::pull(perc_nondir_sig),0)`%) of the effects were revealed to be non-directional. Both tests found significant visual search in [@adam2021a] and multistory integration effects in all three effects from [@battich2021a], while the absolute effect size test also revealed an additional non-directional effect on visual search [@forti2008a], and another effect on spatial attention [@estes2008a]. Notably, the absolute effect-size effect revealed more significant non-directional findings in all three categories, consistent with its superior sensitivity (see sensitivity analysis in Appendix E and H).  Across all categories, the proportion of non-directional effects is considerably higher than the `r round(all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Unconscious Processing") %>% dplyr::pull(perc_nondir_sig),0)`% of the unconscious processing effects (N = `r all_datasets_absES$sig_percent_nsdir_per_type %>% filter(type =="Unconscious Processing") %>% dplyr::pull(N)` according to both tests), as reported above (see Figure 4). These results validate the potential of using non-directional tests to reveal effects on cognition and perception. In striking contrast to the absence of hidden effects in the field of unconscious processing, we found compelling evidence for pronounced inter-individual differences that mask group-level effects in other domains.

However, special care should be taken when interpreting non-directional test results, and when designing experiments targeting non-directional effects (see Box A for best-practice recommendations). Crucially, whenever some nuisance factors are counterbalanced between participants, non-directional effects cannot be uniquely attributed to the independent variable of interest. In such cases, the presence of reliable within-participant effects may reflect the opposing effects of the counterbalanced factor in each group. A case in point can be found in Battich et al., [-@battich2021a], who examined the hypothesis that joint attention affects multisensory integration. Critically, this hypothesis was tested by comparing two social conditions that were counterbalanced across participants, such that for half of the participants a joint attention condition was performed before a baseline condition where participants performed the same task individually, and vice versa for the other half. As a result, contrasting the two conditions within participants is identical to contrasting early and late trials. Thus, although the interaction between social condition and multisensory integration showed significant non-directional effects (sign consistency: SC = `r round(100* battich_res$signcon.statistic[1])`%, p `r apa_p(battich_res$test_result[1])`, SC = `r round(100* battich_res$signcon.statistic[2])`%, p `r apa_p(battich_res$test_result[2])`, and SC = `r round(100* battich_res$signcon.statistic[3])`%, p `r apa_p(battich_res$test_result[3])`; absolute effect size (here using $\phi$ to estimate effect sizes on response category on the first two effects): |ES| = `r round(battich_res$absolute_es.statistic[1],3)`, p `r apa_p(battich_res$absolute_es.p[1])`, |ES| = `r round(battich_res$absolute_es.statistic[2],3)`, p `r apa_p(battich_res$absolute_es.p[2])`, and |ES| = `r round(battich_res$absolute_es.statistic[3],2)`, p =`r apa_p(battich_res$absolute_es.p[3])`, for two categorical response effects and the single RT effect that showed non-directional effects paralleled with null results according to directional analysis), we cannot unambiguously interpret these results as suggesting a causal, non-directional, effect of the social manipulation. This is because, under this design, the social setting condition and the order of experimental conditions are perfectly correlated within individual participants, rendering both potential drivers behind the observed effect.

Similarly, the great majority of experiments in the Confidence Database showed significant non-directional effects of response on confidence, such that individual participants were more confident in making one response while others were more confident when making the other. Specifically, the common task used in the examined experiments involves performing a perceptual decision (e.g., discriminating whether a presented Gabor was oriented to the left or to the right by responding with specific keys), and then rating the level of confidence in the perceptual response (e.g., indicating having high confidence in their previous response that the presented Gabor was oriented to the right). Here, order effects are not a concern, as the two responses are expected to be equally distributed within a block. However, since stimulus-response mapping was not counterbalanced within participants, we are unable to tell whether these effects reflect individual differences in stimulus preferences (e.g., enhanced sensory encoding for right-tilted or left-tilted gratings among different participants) or in response biases (e.g., confidence is systematically higher after reporting a decision with the index finger). Such a response bias may reasonably emerge if, for example, both “tilted right” and “high confidence” responses are made using the right finger of the right and left hands, respectively, and if a right-finger response of the right hand primes a right-finger response of the left hand.

As a general principle, counterbalancing of confounding experimental variables can be done either between participants (for example, using a different response-mapping for odd and even participants) or within participants (for example, changing the response-mapping between experimental blocks for all participants). While both approaches are effective in protecting against confounding of the mean tendency of the dependent measures, only within-subject counterbalancing is effective when testing for non-directional effects. Accordingly, unless all confounding variables (e.g., condition order or response-mapping) are randomized within participants, the interpretation of non-directional effects cannot be uniquely linked to causal effects of the experimental manipulation.

Importantly, although we cannot conclusively attribute these non-directional effects to social setting versus condition order in the first example, or to response versus stimulus in the second, they both constitute examples of true effects that were masked by inter-individual differences. The absence of a directional effect in Battich et al. is indicated by the fact that on average, participants showed similar levels of multisensory integration in the first and second parts of all three experiments showing non-directional effects (`r apa_print(res_battich_resp1_t)$full_result`, `r apa_print(res_battich_resp2_t)$full_result`, and `r apa_print(res_battich_rt2_t)$full_result`). In the case of confidence effects, response mapping was not counterbalanced across participants in many of the considered datasets. This way, the absence of a directional effect of response is also indicative of the absence of a directional effect of stimulus. Together, these previously hidden non-directional findings make the absence of significant non-directional effects in unconscious processing a more convincing indication of the true absence of such effects at the individual-participant level. 

**Figure 4**

*Positive Control: Reanalysis of Effects From Other Fields*

(ref:figure4caption) The results of the sign consistency (left) and absolute effect size tests (right) for null directional effects from different cognitive psychology fields. Blue rhombuses and green squares indicate the outcomes for datasets from the Confidence Database [@rahnev2020a] that were analysed to reveal differences in confidence and metacognitive sensitivity between responses, respectively. Yellow triangles indicate the outcomes for effects from various cognitive psychology studies. Finally, for comparison purposes, we also plot here in pink the results of the studies on unconscious processing (N=`r n_empiric_ns`; circle markers), reported in the previous section. Finally, under the null hypothesis of no non-directional effects, the distribution of p-values should be uniform between 0 and 1. The gray dashed line indicates this expected uniform distribution of p-values (log transformed). Lower panels: each point depicts the $log_{10}$ transformed p-values obtained by the tests (x-axis) and a directional sign-flipping test (y-axis; datasets were filtered to exclude significant directional effects, hence the minimal directional p-value for all datasets is $\alpha=.05$). Upper panels: The p-values density distributions that summarize the results in the lower panel for datasets in each field. 
```{r figure4, out.width="100%", fig.cap = "(ref:figure4caption)"}
# figure 4 was edited in Illustrator according to the svg file generated by sourcing the figure4.R script, the raw figure is located in 'figures//plots'
knitr::include_graphics("figures//figure4.PNG")
```

# Discussion

What is the scope and depth of unconscious processing? Previous claims about high-level unconscious processing effects have recently been criticized for methodological reasons [@shanks2017a; @meyen2022a; @rothkirch2017a], and for lack of replicability [@biderman2018a; @moors2016a; @moors2018a; @hesselmann2015a; @stein2020a]. Here, we point out that testing for effects that are consistent across individuals may be overly conservative for the question at stake. Instead, we examined if these null results might still be underlied by an effect, yet a non-directional one. That is, we tested the hypothesis that individual differences in unconscious processing mask true unconscious effects in individual participants. Adopting a non-directional approach that is robust to inter-individual differences in effects, we used a Bayesian test [@rouder2021a], two frequentist tests based on prevalence assessment and ANOVA [@donhauser2018a; @miller2018a, respectively], and a novel non-parametric frequentist test. We examined previously reported non-significant results (N = `r n_empiric_ns`), and showed they cannot be explained by inter-individual differences in effects. All tests converged on a similar picture: besides three effects that were picked up by two of the five methods (two effects according to each test), unconscious processing effects were not masked by substantial inter-individual differences.

It is important to note that our claim here is not about the presence of individual differences in unconscious processing in general, but about the likelihood that such differences in effect signs may be responsible for null group-level findings. Indeed, previous studies revealed inter-individual differences in the magnitude of unconscious processing effects [@gaal2011a; @cohen2009a; @boy2010a]. For example, Van Gaal et al., [-@gaal2011a] used fMRI and a meta-contrast masked arrows-priming task, to show that grey matter density is correlated with the size of unconscious motor priming effects. Yet importantly, in this experiment effects were defined according to the assumption that *incongruent* trials are performed slower than *congruent* trials (trials in which primes and targets pointed to opposing and the same direction, respectively). This assumption of group coherence in effect signs is prevalent in consciousness science, and in cognitive science more broadly, with few exceptions (for example, see Bolger et al., [-@bolger2019a] for a study where the direction of face orientation effects was not assumed in advance). Here, in contrast, we asked whether relaxing the assumption of effect sign uniformity could reveal unconscious effects that remain undetected using standard directional approaches.

Overall, our non-directional tests detected an effect that was missed by a standard, directional test only in three out of `r n_empiric_ns` datasets (two according to each proposed test). However, even these effects should be examined cautiously. First, neither effect survived a correction for false discovery rate for each test among unconscious processing effects (@benjamini1995a; for the two sign consistency effects: SC = `r round(100* bm_3_res %>% pull(signcon.statistic) %>% first(),0)`%, uncorrected p = `r apa_p(bm_3_res %>% pull(signcon.p) %>% first())`, corrected p = `r apa_p(bm_3_corrected_p_sc)`, for the third experiment in Biderman & Mudrik [-@biderman2018a], and SC = `r round(100* skora_1_res %>% pull(signcon.statistic) %>% first(),0)`%, uncorrected p = `r apa_p(skora_1_res %>% pull(signcon.p) %>% first())`, corrected p = `r apa_p(skora_1_corrected_p_sc)`; for the absolute effect size test: for the first and second experiments in Skora et al., [-@skora2021a], |ES| = `r round(skora_1_res %>% pull(absolute_es.statistic) %>% first(),2)` and |ES| = `r round(skora_2_res %>% pull(absolute_es.statistic) %>% first(),2)`, uncorrected ps = `r apa_p(skora_1_res %>% pull(absolute_es.p) %>% first())` and `r apa_p(skora_2_res %>% pull(absolute_es.p) %>% first())`, corrected ps = `r apa_p(skora_1_corrected_p_abs_es)` and `r apa_p(skora_2_corrected_p_abs_es)`). Hence, these effects may reflect a type-1 error. Furthermore, the more powerful absolute effect size test did not detect the scene-object congruency effect from [@biderman2018a] as was the case for the other three tests. Lastly, Skora and colleagues expressed concerns regarding possible contamination of their measured effect by conscious processing due to regression to the mean [@shanks2017a], suggesting that  participants' awareness may have been underestimated. This concern was further confirmed in a reanalysis of objective measures of awareness using a test tailored to yield high power in detecting awareness (Lublinsky et al., in preparation), finding significant awareness in both of their experiments. Thus, our findings may be interpreted as suggesting no masking of unconscious processing effects by population heterogeneity.

Relatedly, our investigation aimed at unmasking non-directional unconscious processing effects. Accordingly, we did not examine whether such effects also exist in direct measures of awareness such as the ability of participants to correctly identify the prime stimulus in a forced-choice task [@reingold1988a]. In principle, group-level chance performance in an objective measure of awareness may reflect the joint effect of participants who systematically perform above and below chance. If so, non-directional effects in the objective measure may also go unnoticed by researchers, leading them to underestimate awareness and incorrectly infer unconscious processing.

However, unlike unconscious processing effects, where effect signs are irrelevant to the theoretical question at stake, systematic below-chance performance in objective tasks is harder to interpret. Indeed, below-chance accuracy can be equally taken as a sign of awareness, or as a sign of unawareness (see @klauer1998a; @skora2023a). Furthermore, the reanalysis of empirical data reported in Lublinsky and colleagues (N = 79 awareness measures; in preparation), revealed only scarce evidence for non-directional effects on awareness (as indicated above, two stark exceptions to this conclusion are the two measures used in Skora et al., [-@skora2021a]. Thus, while our focus here was on non-directional effects in indirect measures of consciousness, testing for the presence of such effects in direct, or objective, measures, is also of critical theoretical importance.

Notably, the homogeneity of effect signs is assumed under all common dissociation paradigms used to study unconscious processing, including the double dissociation and sensitivity paradigms which had been designed to relieve other strong assumptions underlying the study of unconscious effects (@schmidt2006a; @meyen2022a).  Our results inform these paradigms as well as the standard paradigm which was used in the studies we reanalyzed here. Specifically, both the double and sensitivity dissociation paradigms assume a consistent direction between the effects in the direct and indirect tasks. Accordingly, the lack of evidence for reversed effects reported here supports the validity of these paradigms for demonstrating unconscious effects.

While our focus here was on unconscious processing, a non-directional analysis approach can be useful in many fields of investigation where individual differences are expected. A null finding in a standard t-test or an ANOVA may indicate the true absence of an effect or a lack of statistical power, but it may also be driven by qualitative heterogeneity in participant-level effect signs. In the field of neuroimaging, the adoption of information-based, non-directional approaches famously revealed such effects that were otherwise masked by heterogeneity in neural activation patterns and fine brain structure [@norman2006a; @kriegeskorte2013a; @gilron2017a; @ince2021a; @ince2022a]. In the context of this investigation, we found considerable evidence for cases where inter-individual differences mask group-level effects. These cases carry theoretical significance both in uncovering previously missed effects, and in revealing aspects of human cognition that are subject to considerable population variability [@rouder2020a; @rouder2021a; @bolger2019a].

Previously, Rouder & Haaf [-@rouder2021a] suggested that such qualitative individual differences may be expected in preference or bias-based effects (e.g., @schnuerch2021a; @rouder2021a), but not in effects that are driven by low-level perceptual and attentional processes. Consistent with this proposal, the absence of substantial evidence for variability in effect signs in unconscious processing was paralleled with strong evidence for such qualitative inter-individual differences in subjective confidence ratings (e.g., some participants are more confident in classifying a grating as oriented to the right, while others show the opposite preference)^[As we note above, since in most of these studies responses and stimuli are closely correlated, these effects cannot be unambiguously attributed to stimulus preferences or response priming effects. Relatedly, more recent work reveals that such inter-individual differences in preference for specific responses or stimuli can be traced back to heterogeneity in sensory encoding @rahnev2021a]. However, robust participant-level effects were masked by qualitative individual differences in other domains too, not all of them relate to higher-level preferences or biases. For example, non-directional effects of distractor presence were found in visual search experiments (@adam2021a; SC > `r round(100*min(ns_adam_res$signcon.statistic))`%, p $<$`r apa_p(ceiling(max(ns_adam_res$test_result * 100))/100)`, for `r xfun::numbers_to_words(nrow(ns_adam_res))` out of `r xfun::numbers_to_words(nrow(all_ns_adam_res))` measured effects, only one of them was significant according to the absolute effect size test: |ES| = `r round(ns_adam_res[ns_adam_res$absolute_es.p < alpha,]$absolute_es.statistic,2)`, p `r apa_p(ns_adam_res[ns_adam_res$absolute_es.p < alpha,]$absolute_es.p)`). Moreover, the absolute effect size test (but not the sign consistency test) revealed significant effects in two studies from the Reproducibility Project: words associated with locations had a non-directional effect on spatial attention (|ES| = `r round(Estes_res$absolute_es.statistic,2)`, p = `r apa_p(Estes_res$test_result)`; @estes2008a), and visual search was non-directionally affected by the interaction between target location and viewpoint prototypicality (|ES| = `r round(Forti_res$absolute_es.statistic,2)`, p = `r apa_p(Forti_res$test_result)`; @forti2008a). These findings echo the non-directional effects of distractor-target compatibility on action planning, previously revealed by @miller2018a using their OANOVA test on data from Machado et al (2007, 2009). When re-analysing Machado’s data using our non-parametric tests, we find similar results: null directional effects, accompanied by significant non-directional effects (sign consistency: SC = `r round(100* machado_2007_350$sc_statistic)`%, p `r apa_p(machado_2007_350$sc_p)`  and absolute effect size: |ES| = `r round(machado_2007_350$abs_es_statistic,2)`, p `r apa_p(machado_2007_350$abs_es_p)`, for a target-distractor SOA of 350ms in @machado2007a; sign consistency: SC = `r round(100*machado_2009_650$sc_statistic)`%, p = `r apa_p(machado_2009_650$sc_p)` and absolute effect size: |ES| = `r round(machado_2009_650$abs_es_statistic,2)`, p `r apa_p(machado_2009_650$abs_es_p)`, for an SOA of 650ms in @machado2009a). Thus, aside from shedding light on previous non-significant results, our preliminary findings inform previous claims regarding the plausibility of population heterogeneity in effect signs in perceptual and attentional effects in general, providing some indication that such effects may be more prevalent than previously assumed.

To facilitate the adoption of this non-directional approach in experimental psychology, we release with this paper an R package with a simple-to-use implementation of our error-controlled and non-parametric sign consistency test ([https://github.com/mufcItay/signcon](https://github.com/mufcItay/signcon)). We note that unlike directional tests, the validity of non-directional tests depends on counterbalancing of confounding variables not only across participants, but also across trials within a single participant. We recommend using these tests to complement standard, directional tests, taking into account the effect of additional tests on the family-wise error rate. Furthermore, although the test revealed effects in various domains, special attention should be given to statistical power when collecting data for a non-directional test, considering both the number of participants and the number of trials per participant. This is especially important when the effect size of interest is small, as is clearly the case in unconscious processing studies (more generally, when the relation between true variability between participants and measurement error is small; see @rouder2023a and Appendix H). Given proper use, the test should be particularly useful in interpreting null findings at the group level (see Box A for a more detailed description of best-practice recommendations for non-directional testing). This seems highly relevant to the field of unconscious processing, where null results are becoming more prevalent, and carry theoretical significance as hinting at possible functional roles for conscious processing.

# Conclusions

Experimental demonstrations of unconscious processing have been reported for nearly 150 years now (e.g., @peirce1884a), yet their reliability and robustness have repeatedly been put into question (e.g., @holender1986a and @shanks2017a). Here, we examined the possibility that some of the findings against such processing, reporting null results, might hide effects at the individual level, yet in opposing directions. We employed five non-directional tests to re-examine `r n_empiric_ns` null effects. Our findings suggest no role for individual differences in explaining non-significant effects at the group level. Furthermore, by expanding our exploration outside the domain of unconscious processing, we found compelling evidence for effects that were shadowed by individual differences in effect signs, nuancing views about the universality of cognitive and perceptual effects. We provide a user-friendly implementation of the non-directional tests, and recommend their use for interpreting null results. 

\newpage

## Data and code availability
  All data except for the datasets from Machado et al., 2007, 2009, for which participants did not consent to having their data shared online, and all analysis code are available at [https://github.com/mufcItay/NDT](https://github.com/mufcItay/NDT), using `r cite_r(file = "r-references.bib")`.

## Acknowledgements
  We thank all researchers who shared their data with us: Doby Rahnev, Lucas Battich, Kirsten C. S. Adam, Natalie Biderman, Nathan Faivre, Sung-En Chien, Lina Skora, Fenja Mareike Benthien, Iris Zerweck, Mikko Hurme, Timo Stein, Liana Machado, and the Open Science Collaboration. Furthermore, we thank David Shanks, Miguel A. Vadillo, Thomas Schmidt and Sascha Meyen for their feedback on an early version of this work. MM was funded by a post-doctoral research fellowships from All Souls College at the University of Oxford.

  
\newpage

# References

::: {#refs custom-style="Bibliography"}
:::

\newpage
``` {r appendix-A, warning = FALSE, include=FALSE}
source('appendix\\Appendix_A.R')
```

# Appendix A. Simulating non-directional unconscious processing effects with Wald distributions {#Apndx-A}

To complement the simulation of normally distributed RTs reported in the main text (under the section titled 'Simulating non-directional unconscious effects'), we ran an additional simulation using more realistic Wald distribution. Importantly, unlike normal distributions and similarly to RT distributions, Wald distributions are strictly positive and right-skewed. Again, we simulated a *non-directional differences* scenario, and a *global null* scenario, which differed by the within-participant shape parameter $\lambda$ ($\lambda$=`r get_wald_lambda(nde_mu, nde_sd_w)` and $\lambda$=`r get_wald_lambda(sn_mu, sn_sd_w)` in the former and later scenarios, respectively). Accordingly, RTs were sampled from shifted Wald (SW) distributions ($RT_{i,c} \sim {\mathcal{SW}} (\mu + c*e_{i}, \lambda, \tau)$, where $\mu$ was set to `r wald_mu`ms and $\tau$ was set to `r wald_tau`ms, to mimic typical RT distributions).

First, as in the original analysis, a t-test did not find an effect in neither scenario (*non-directional differences*: `r apa_print(wald_nde_t_test)$full_result`; *global null*: `r apa_print(wald_sn_t_test)$full_result`). More crucially, as can be seen in Table A1, the results of all probed tests were similar to those obtained with normally distributed RTs, finding evidence for an effects only in the non-directional scenario.


``` {r appendix-A-table}
# create the Wald results appednix table according to apndxA script outputs
Test <- c('GNT', 'QUID', 'OANOVA', 'Sign-Consistency', 'Absolute Effect Size')
gnt_format_res <- function(res) {paste0(round(100 * res$stat,0), '% significant effects, p ', apa_p(res$p))}
QUID_format_res <- function(res) {paste0('BF = ', ifelse(1/res$quid_bf > 10^5, formatC(1/res$quid_bf, format = "e", digits = 2), round(1/res$quid_bf,2)))}
OANOVA_format_res <- function(res) {paste0('F(',res$df_num ,',',res$df_denom ,') = ', round(res$F,2),', p',ifelse(res$p > .001,'= ', ' '), apa_p(res$p))}
sc_format_res <- function(res) {paste0('SC = ', round(100 * res$statistic,0), '%, p',ifelse(res$p > .001,'= ', ' '), apa_p(res$p))}
abses_format_res <- function(res) {paste0('|ES| = ', round(res$statistic,2), ', p',ifelse(res$p > .001,'= ', ' '), apa_p(res$p))}

ND_Results <- c(gnt_format_res(wald_nde_gnt_res), QUID_format_res(wald_nde_quid_res), OANOVA_format_res(wald_nde_OANOVA_res), sc_format_res(wald_nde_sc_res), abses_format_res(wald_nde_abs_es_res)) # Non-directional effect scenario
GN_Results <- c(gnt_format_res(wald_sn_gnt_res), QUID_format_res(wald_sn_quid_res), OANOVA_format_res(wald_sn_OANOVA_res), sc_format_res(wald_sn_sc_res), abses_format_res(wald_sn_abs_es_res)) # Non-directional effect scenario
apndxA_results_table_df <- data.frame(Test, ND_Results, GN_Results)
names(apndxA_results_table_df) <- c('Test', 'Non-directional effects scenario', 'Global Null scenario')
# print the table according to apa style using a small font size
apa_table(
  apndxA_results_table_df,
  caption = "Table A1. Resutls for simulated data under the non-directional effect and global null scenarios using Wald distributions",
  font_size = 'footnotesize')
```


\newpage

# Appendix B {#Apndx-B}
``` {r appendix-B}
# create the appednix table by code
Study <- c('Biderman & Mudrik, 2018', 'Faivre et al., 2014', 'Stein & Peelen, 2021', 
'Zerweck et al., 2021', 'Benthien & Hesselmann, 2021', 'Hurme et al., 2020', 
'Skora et al., 2021', 'Chien et al., 2022')
Labels <- c('BM1-3', 'F1-8', 'SVP1-5','Z1-7','BH1','H1-4','S1-2','C1-3')
Topic <- c('Scene congruency', 'Multisensory integration', 'Location effects + PAS (detection)',
'Numerical Priming', 'Numerical Priming', 'Colours', 'Instrumental Learning', 'Semantic priming')
Paradigm <- c('Masking', 'Masking', 'CFS', 'Masking', 'CFS', 'TMS + Metacontrast Masking', 'Masking', 'CFS')
DV <- c('RT', 'RT', 'd\'', 'RT', 'RT', 'RT', 'd\'', 'RT')
Notes <- c('Replication study. For all experiments, log(RT) was used in the original analysis',
'Four experiments, with two effects in each experiment (identical/different targets). For all experiments, log(RT) was used in the original analysis',
'Two experiments (3 and 4 in the paper), measuring effects in different prime-mask SOAs',
'Two experiments (2 and 3 in the original paper), measuring effects in different SOA / Contrast conditions',
'Interaction effect - prime congruency X location certainty',
'Redundant target effect (TMS / Masking X Blue / Red)',
'Regression to the mean as a confound according to authors',
'Word, Picture, and trait discrimination tasks')
studies_df <- data.frame(Study, Labels, Topic, Paradigm, DV, Notes)
# print the table according to apa style using a small font size
apa_table(
  studies_df,
  caption = "Unconscious processing effects metadata",
  font_size = 'footnotesize')
```

\newpage


# Appendix C. GNT, QUID and OANOVA results for datasets showing a directional effect {#Apndx-C}

(ref:appendixccaption) *Note*. The results of applying the GNT (A), QUID (B) and OANOVA (C) tests to effects that produced significant results in a non-parametric directional test. Same conventions as Fig. 2.
```{r appendix-C, out.width="100%", fig.cap="(ref:appendixccaption)", warning = FALSE}
knitr::include_graphics("figures//plots//effect_available_methods_res.png")
```

\newpage

``` {r appendix-D, warning = FALSE}
# use the results of running the appendixD script above, analyze the results for QUID and OANOVA separately
appendix_D_analysis <- analyze_appendix_D(appendixD_res)
quid_apndx_res <- appendix_D_analysis$quid_res
oanova_apndx_res <- appendix_D_analysis$oanova_res
# extract simulation parameters for each scenario (FAs and Sensitivity analysis)
apndxD_sigma_w_low <- min(apndxD_conf_FAs$params$sigma_w)
apndxD_sigma_w_unequal <- max(apndxD_conf_FAs$params$sigma_w)
apndxD_FA_sigma_b <- dplyr::first(apndxD_conf_FAs$params$sigma_b)
apndxD_sensitivity_sigma_b <- dplyr::first(apndxD_conf_sensitivity$params$sigma_b)
apndxD_FA_Np <- dplyr::first(apndxD_conf_FAs$params$N_p)
apndxD_sensitivity_Np <- dplyr::first(apndxD_conf_sensitivity$params$N_p) 
apndxD_mu <- dplyr::first(apndxD_conf_FAs$params$mu)
# we multiply by 2 because the simulation expects number of trials per condition and we report the total amount of trials
apndxD_Nt <- 2 * dplyr::first(apndxD_conf_FAs$params$N_t)
apndxD_iterations <- max(apndxD_conf_FAs$results$seeds)
```

# Appendix D. Violating the equal within-individuals variance assumption {#Apndx-D}
We used the simulation scheme described in the main text (see section ‘Simulating non-directional unconscious effects’), to test the consequences of violating the equal within-individuals variance assumption for both QUID and the OANOVA test. We compared the distribution of Bayes factors and p-values obtained by applying QUID and the OANOVA test to generated data meeting and violating the equal within-participants variance assumption. In the first, equal-variance case, the within-individual standard deviation was low ($\sigma_{w}=`r apndxD_sigma_w_low`$) for all participants. In the second, unequal-variance case, the within-individual standard deviation was low ($\sigma_{w}=`r apndxD_sigma_w_low`$) for all participants except one, for whom it was set to a high value ($\sigma_{w}=`r apndxD_sigma_w_unequal`$). As in the main simulation, the effect sizes of each participant were sampled from a normal distribution centred at zero ($e_{i} \sim {\mathcal{N}}(`r apndxD_mu`, \sigma_{b})$; where $e_{i}$ denotes the effect size of the $i^{th}$ participant). Within this framework, we examined two scenarios: a *non-directional differences* scenario where participants are differentially affected by the experimental manipulation ($N_{p}=`r apndxD_sensitivity_Np`$, $\sigma_{b}=`r apndxD_sensitivity_sigma_b`$), and a *global null* condition where all participants are unaffected by the experimental manipulation ($N_{p}=`r apndxD_FA_Np`$, $\sigma_{b}=`r apndxD_FA_sigma_b`$). In both scenarios, we simulated random data in `r apndxD_iterations` iterations, and used the same number of trials per condition (the total number of trials, $N_{t}= `r apndxD_Nt`$).

To examine the tests’ specificity, we measured the proportion of iterations where evidence for an effect was erroneously found in the global null condition. In the equal-variance case, all iterations provided evidence for the lack of an effect according to QUID (all BFs < $\frac{1}{`r bf_criteria`}$). Similarly, non-significant results were found by the OANOVA test in `r round(oanova_apndx_res %>% filter(Analysis == sigma_b_FAs, Condition == sigma_w_equal,sig_OANOVA == FALSE) %>% pull(sig_prop))`% of the iterations. However, in the unequal-variance case, false-positives were obtained in `r round(quid_apndx_res %>% filter(Analysis == sigma_b_FAs, Condition == sigma_w_unequal,sig_QUID == 'H1') %>% pull(sig_prop))`% of the QUID Bayes Factors (BF > `r bf_criteria`), and `r round(quid_apndx_res %>% filter(Analysis == sigma_b_FAs, Condition == sigma_w_unequal,sig_QUID == 'Inconclusive') %>% pull(sig_prop))`% showed inconclusive evidence. Again, the OANOVA test showed a similar pattern, detecting falsely significant effects in `r round(oanova_apndx_res %>% filter(Analysis == sigma_b_FAs, Condition == sigma_w_unequal,sig_OANOVA == TRUE) %>% pull(sig_prop))`% of the iterations. Thus, we show that the specificity of these tests is compromised by violations of the equal-variance assumption.

We then analyzed the tests’ outcomes in the non-directional differences scenario to examine their sensitivity. When the equal-variance assumption was met, both tests found evidence for an effect (all BFs > `r bf_criteria`, and all p-values < `r apndxD_alpha`). In contrast, in the unequal-variance case, only `r round(quid_apndx_res %>% filter(Analysis == sigma_b_sensitivity, Condition == sigma_w_unequal,sig_QUID == 'H1') %>% pull(sig_prop))`% of QUIDs BFs showed evidence for an effect, whilst `r round(quid_apndx_res %>% filter(Analysis == sigma_b_sensitivity, Condition == sigma_w_unequal,sig_QUID == 'H0') %>% pull(sig_prop))`% showed evidence for no effect (the remaining `r round(quid_apndx_res %>% filter(Analysis == sigma_b_sensitivity, Condition == sigma_w_unequal,sig_QUID == 'Inconclusive') %>% pull(sig_prop))`% were inconclusive). Similarly, the OANOVA test found significant effects in only `r round(oanova_apndx_res %>% filter(Analysis == sigma_b_sensitivity, Condition == sigma_w_unequal,sig_OANOVA == TRUE) %>% pull(sig_prop))`% of the iterations. Hence, both tests missed true effects when the assumption was not met, demonstrating that their sensitivity is compromised by violations of the equal-variance assumption.

\newpage


```{r appendix_E}
# use the results of running the appendixE script above, to extract the simulation parameters
apndxE_sig_b <- dplyr::first(apndx_e_conf$params$sigma_b)
apndxE_sig_w <- dplyr::first(apndx_e_conf$params$sigma_w)
# we multiply by 2 because the simulation expects number of trials per condition and we report the total amount of trials
apndxE_Nt <- 2 * unique(apndx_e_conf$params$N_t)
apndxE_Np <- unique(apndx_e_conf$params$N_p)
apndxE_de_mu <- max(apndx_e_conf$params$mu)
apndxE_nd_mu <- min(apndx_e_conf$params$mu)
apndxE_iterations <- max(apndx_e_conf$results$seeds)
```


# Appendix E. Comparing the power of the GNT and the proposed tests {#Apndx-E}

To examine the sensitivity of GNT and compare it with both the sign consistency and absolute effect size tests, we conducted a power analysis, simulating two scenarios under the simulations scheme described in the main text (see section ‘Simulating non-directional unconscious effects’): First, a *non-directional differences* scenario where an effect exists for each participant but it is inconsistent within participants ($e_{i} \sim {\mathcal{N}}(`r apndxE_nd_mu`, \sigma_{b})$; where $\sigma_{b}$=`r round(apndxE_sig_b,1)`). Second, a *directional effect* scenario, with individual variation around a positive mean effect size ($e_{i} \sim {\mathcal{N}}(`r apndxE_de_mu`, \sigma_{b})$; where $\sigma_{b}$=`r round(apndxE_sig_b,1)`). For each scenario, we manipulated the number of simulated participants ($N_{p}$=`r apndxE_Np[1]`/`r apndxE_Np[2]`/`r apndxE_Np[3]`) and trials ($N_{t}$=`r apndxE_Nt[1]`/`r apndxE_Nt[2]`/`r apndxE_Nt[3]`) across `r apndxE_iterations` random iterations, with the within-participant SD ($\sigma_{w}$) set to `r apndxE_sig_w` in both scenarios.
Statistical power was defined as the proportion of significant results for each test ($\alpha = .05$). While all tests were similarly sensitive when applied to well-powered datasets (e.g., when $N_{p}$=`r apndxE_Np[3]` or $N_{t}$=`r apndxE_Nt[3]`), both the sign consistency test, and yet more so, the absolute effect size test proved to be more sensitive in the remaining conditions (see [@baker2021] for a more comprehensive power analysis of a directional test in the *directional effect* scenario).

**Figure E1**

*Comparing the Sensitivity of the Proposed Tests and GNT *

*Note*. Comparing the sensitivity of the proposed tests and GNT. Power analysis for the absolute effect size (ABSES), sign consistency (SC) and the global null (GNT) tests for simulated datasets. Top panel: Each cell depicts the percent of iterations where ABSES, SC and GNT resulted in significant effects (upper and lower panels, respectively). Rows and columns correspond to the number of simulated participants ($N_{p}$), and the number of simulated trials per participant ($N_{t}$), respectively. Left panel: the results of the ABSES, GNT and SC in the *directional effect* scenario. Right: the results of the ABSES, GNT and SC in the *non-directional differences* scenario. The number in each cell denotes the % of significant effects (power) in each simulated condition (across 1000 iterations), and darker blue colors indicate higher power.

```{r appendix-E, out.width="100%", warning = FALSE}
knitr::include_graphics(paste("appendix","Appendix_E.png", sep =
                                .Platform$file.sep))

```

\newpage

# Appendix F. Extending the sign consistency test to additional use cases {#Apndx-F}

We provide two code examples to demonstrate how to extend the sign-consistency test implemented in the signcon R package ([https://github.com/mufcItay/signcon](https://github.com/mufcItay/signcon)) to common use case: F.1. A 2 X 2 interaction, and F.2. calculating SDT's d'. For both examples we will use simulated trial-level data, mimicking a 2X2 within-participant design.


```{r appndxDdata}
# number of participants
N <- 15
# number of trials under each condition of the independent variable 
n <- 30
# generate the data:
# participant identifier variable
idv <- rep(1:N, each = n*2)
# independent variable (0 = condition1, 1 = condition2)
iv <- rep(rep(c(0,1), each = n), N)
# second independent variable (0 = condition3, 1 = condition4)
iv2 <- rep(rep(c(0,1), n), N)
# dependent variables:
# a response variable, which is a function of both 'iv' and 'iv2'
dv_response <- rbinom(N*n*2,1,iv * .25 + iv2 * .1)
# an RT variable, sampled from a normal distribution which is a function of the interaction between 'iv' and 'iv2' (RT_iv,iv2 ~ N(500 + 50 *iv*iv2, 50))
dv_RT <- rnorm(N*n*2, 500 + 50 * iv * iv2, 50)
# generate a dataframe with all variables, with the independent and id variables as factors
data <- data.frame(idv, iv, iv2, dv_response, dv_RT)
factor_vars <- c('idv','iv', 'iv2')
data[factor_vars] <- lapply(data[factor_vars], factor)
head(data)
```

## Appendix F.1: 2X2 interction effect:
To test for an interaction effect we override the default test-consistency summary function. In this example, we use a function that summarizes RTs ('dv_RT') by calculating the mean difference between the two conditions of the second variable ('iv2'). This summary function will be applied to the dependent variable ('dv_RT') of different splits of the data, under each condition of the independent variable ('iv'), when calculate sign-consistency scores per participant.

```{r appndxDinteraction_usecase, echo = TRUE, eval=FALSE}
# the interaction summary function for an interaction effect
interaction_summary_function <- function(data) {
  if(length(unique(data$iv2)) != 2) { 
    return(NA)
  }
  # we use mean to summarize the RT under each level of 'iv2'
  res <- mean(data[data$iv2 == 1,]$dv_RT) - 
    mean(data[data$iv2 == 0,]$dv_RT)
  return(res)
}
# run the sign-consistency test
sc_interaction <- test_sign_consistency(data, idv = 'idv', iv = 'iv', dv = c('iv2', 'dv_RT'), summary_function = interaction_summary_function)
```


## Appendix F.2: d' effect:
To test for an effect on sensitivity (d') we override the default test-consistency summary function, to compute the normalized rate of responses given for a reference stimulus (here, the reference stimulus is encoded as '1'). As explained above, since this summary function is applied to each condition and participant under the independent variable ('iv') when calculating sign consistency scores, the sign-consistency test would test for consistent d' sign between different splits of the data for the respective participant.

```{r appndxDd_prime_usecase, echo = TRUE, eval=FALSE}
# since in this use case there is only one dependent variable, the 'data' argument
# is a vector containing all of the dv_response values for the sampled split
dprime_summary_function <- function(data) {
  # count how many '1' responses were given
  cnt <- sum(data)
  # get the total number of trials in this split
  len <- length(data) 
  # correction for edge cases where participants only give one response (0 / 1)
  floor_rate <- 1/(2*len)
  ceiling_rate <- 1 - 1/(2*len)
  # calculate the observed rate of 1 responses
  rate <- ifelse(cnt == 0, floor_rate, 
                 ifelse(cnt == len, ceiling_rate, 
                        cnt / len))
  return (qnorm(rate))
}
# run the sign-consistency test
sc_dprime <- test_sign_consistency(data, idv = 'idv', iv = 'iv', dv = 'dv_response', summary_function = dprime_summary_function)
```


\newpage

# Appendix G. Results of the novel non-parametric tests for datasets showing a directional effect {#Apndx-G}

*Analysis of datasets showing a directional effect*

(ref:appendixgcaption) *Note*. The results of applying the proposed tests for datasets showing a directional effect (N = 7), using the Sign Consistency test (upper panel) and the Absolute Effect Size test (lower panel). The x-axis lists effect labels. Same conventions as Fig. 3.

```{r appendix-G, out.width="100%", warning = FALSE}
knitr::include_graphics("figures//plots//effect_plt_sign_con__res.png")
```

```{r appendix-G-abses, out.width="100%", fig.cap = "(ref:appendixgcaption)", warning = FALSE}
knitr::include_graphics("figures//plots//effect_plt_abs_es__res.png")
```

\newpage

# Appendix H. Empirically informed power estimation for the novel non-parametric tests {#Apndx-H}

To examine whether lack of power can explain not finding convincing evidence for non-directional unconscious processing effects (Fig. 3), we examined the sensitivity of both the sign-consistency and absolute effect size tests in detecting effects of empirically relevant studies.
To that end, we simulated *non-directional differences* scenarios with different degrees of true between participants variance ($\sigma_b \in \{1, 1.5, 2\}$) and fixed amount of within participant variability ($\sigma_w = 10$). These parameters were chosen based on previous works where the ratio $\frac{\sigma_b}{\sigma_w}$ was estimated to values ranging between .04 and .15 (M = .1, SD = .04) in six unconscious processing datasets [@meyen2022a], while in another work estimating the same parameter in 24 cognitive control studies [@rouder2023a] (where no unconscious manipulation was used) values ranged between .05 and .36 (M = .14, SD = .08). Then, we determined the number of trials and participants in the simulated datasets according to the parameters used in the unconscious processing datasets we obtained by calculating the 25%, 50%, and 75% percentile of both parameters. This resulted in additional simulation conditions where the number of participants was set to $N_p \in \{`r apndxH_N_p[1]`, `r apndxH_N_p[2]`, `r apndxH_N_p[3]`\}$ and the total number of trials was set to $N_t \in \{`r apndxH_N_t[1]`, `r apndxH_N_t[2]`, `r apndxH_N_t[3]`\}$. All other simulation parameters were the same as detailed in Appendix E.

From the results of this power simulation we obtained an estimate for the power of both the sign consistency and absolute effect size tests^[Importantly, this decision incorporates the fact that the proposed tests may also detect small directional effects that remain undetected using standard tests, due to heterogeneity in individual-level effects. In this case, estimating the power of the test based solely on the ratio of between and within participant variability might underestimate the true power of the test for the obtained datasets. Hence, we used the same effect size as was done in Meyen et al., 2022, who used it as part of a “benefit of a doubt” approach for a different set of analyses on unconscious processing effects.], in common sample size settings, for an effect of interest of $\frac{\sigma_b}{\sigma_w}=.15$. We then used a prevalence test to test whether the obtained power estimate is compatible with the low observed prevalence of unconscious processing effects we found. Due to the concern regarding contamination by conscious processing in the two effects from Skora et al., 2021 (see the Discussion for more details), we excluded these effects from this analysis, resulting in an observed prevalence of zero out of `r n_tested_effects_abs_es` tested effects (i.e., no non-directional unconscious effects were found). Accordingly, we tested if finding no unconscious effects is surprising relative to the expected prevalence according to our power estimate (here using the power estimate of the absolute effect size test which was highest amongst the two probed tests (`r round(100 * apndx_H_median_cnd_power_abs_es)`%), and assuming that all studies had true non-directional effects). Indeed, this was the case when comparing the observed rate of significant effects with the expected rate if all, or even one-eighth of the tested contrasts had true non-directional effects (N = `r round(low_prev_ratio * n_tested_effects_abs_es,0)` out of `r n_tested_effects_abs_es` tested effects), yet were not detected due to a lack of power ($CI_{95}$ = [`r round(100 * test_full_prevalence$conf.int[1])`, `r round(100 * test_full_prevalence$conf.int[2])`], p`r apa_p(test_full_prevalence$p.value)`, and p = `r apa_p(test_low_prevalence$p.value)`, respectively). Furthermore, even when including the two arguably conscious effects found by Skora and colleagues in the analysis, the prevalence of observed effects remained significantly lower than expected if a third of the effects were true (p = `r apa_p(test_third_prevalence_all_datasets$p.value)`; using as the observed prevalence of effects across all datasets, `r round(100*sc_sig_n/n_empiric_ns)`%). Hence, we interpret these results as suggesting that for an effect size of $\frac{\sigma_b}{\sigma_w}=.15$, the results are unlikely to be explained simply by lack of power.

**Figure H1**

*Data Informed Power Analysis of the proposed tests*

(ref:appendixhcaption) *Note*. Power analysis for various *non-directional differences* scenarios. The results for the sign consistency (SC) and the absolute effect size (ABSES) tests are presented on the upper and lower plots, respectively. Each panel depicts the results for different effect sizes ($\frac{\sigma_b}{\sigma_w} \in \{.1, .15, .2\}$). Within each panel, the x and y axes depict different settings for the number of participants ($N_p$) and the total number of trials across two conditions ($N_t$), both determined according to the 25, 50 and 75 percentiles of these parameters in the unconscious processing datasets we collected. The number in each cell denotes the % of significant effects (power) in each simulated condition (across `r apndx_H_n_iterations` iterations), and darker blue colors indicate higher power.

```{r appendix-H, out.width="100%", fig.cap="(ref:appendixhcaption)", warning = FALSE}
knitr::include_graphics(paste("appendix","Appendix_H_ABSES.png", sep =
                                .Platform$file.sep))

knitr::include_graphics(paste("appendix","Appendix_H_SC.png", sep =
                                .Platform$file.sep))

```

\newpage

```{r apndx_i_statistics, include=FALSE}
apndx_I_N_GNT <- all_datasets_GNT$nsdir_N %>% pull(N) %>% sum()
apndx_I_N_QUID <- all_datasets_QUID$nsdir_N %>% pull(N) %>% sum()
apndx_I_N_OANOVA <- all_datasets_OANOVA$nsdir_N %>% pull(N) %>% sum()
```

# Appendix I. Testing for effects from other domains with the alternative tests {#Apndx-I}

For completeness, we report here the results of the GNT, QUID, and OANOVA tests for all datasets outside the domain of unconscious processing reported on the main text, which were non-significant effects according to a directional test^[Notably, since both QUID and OANOVA were developed for continuous dependent variables, with the default prior settings of QUID were set according to expected patterns for RTs, we did not use either test to analyze metacognitive sensitivity effects, and also excluded confidence effects from the QUID analysis. Similarly, QUID was not used for analyzing interaction effects, because its current implementation does not allow for such analysis.] (see section 'Positive control: Testing within-participant non-directional effects across experimental psychology studies' for the same analysis using the proposed tests). Crucially, this analysis should be interpreted with caution given the results we report in Appendix D and E, showing potential issues with the sensitivity and/or specificity of these tests.

Overall the results were similar to the one found with the proposed tests: First, the vast majority of datasets from the confidence database showed non-directional effects (`r round(all_datasets_GNT$sig_percent_nsdir_per_type %>% filter(type =='Confidence') %>% pull(perc_nondir_sig))`% and `r round(all_datasets_OANOVA$sig_percent_nsdir_per_type %>% filter(type =='Confidence') %>% pull(perc_nondir_sig))`%, for GNT and OANOVA, respectively). Similarly, GNT found metacognitive sensitivity effects in `r round(all_datasets_GNT$sig_percent_nsdir_per_type %>% filter(type == "Metacognitive Sensitivity") %>% pull(perc_nondir_sig))`% of the datasets. Lastly, within the 'Cognitive Psychology' datasets category, both GNT and OANOVA were significant for all three effects from [@battich2021a] on multisensory integration (since all of these effects involve interactions, they were not analyzed using QUID), while obtaining significant results for only one of the two visual-search sign consistency effects by the absolute effect size, OANOVA and QUID tests, or none of them for GNT, and a significant effect in [@estes2008a] according to both OANOVA and GNT in agreement with the significant absolute effect size result for this effect (overall cognitive psychology datasets, GNT, QUID and OANOVA found `r round(all_datasets_GNT$sig_percent_nsdir_per_type %>% filter(type == "Cognitive Psychology") %>% pull(perc_nondir_sig))`%, `r round(all_datasets_QUID$sig_percent_nsdir_per_type %>% filter(type == "Cognitive Psychology") %>% pull(perc_nondir_sig))`%, and `r round(all_datasets_OANOVA$sig_percent_nsdir_per_type %>% filter(type == "Cognitive Psychology") %>% pull(perc_nondir_sig))`% of significant effects, out of `r all_datasets_GNT$nsdir_N %>% filter(type == "Cognitive Psychology") %>% pull(N)`, `r all_datasets_QUID$nsdir_N %>% filter(type == "Cognitive Psychology") %>% pull(N)`, and `r all_datasets_OANOVA$nsdir_N %>% filter(type == "Cognitive Psychology") %>% pull(N)` effects examined by each test).

**Figure I1**

*The Results of the Alternative Tests on Effects From Other Fields*

(ref:appendixicaption) Appendix I-figure 1. The results of applying the GNT (A) (N=`r apndx_I_N_GNT`), QUID (B) (N=`r apndx_I_N_QUID`), and OANOVA (C) (N = `r apndx_I_N_OANOVA`) tests to null directional effects from different cognitive psychology fields. Same conventions are used as in Figure 4 in the main text. Effects that are incompatible with OANOVA or QUID were excluded from this analysis. In panel B the two black vertical indicate BF criteria of 3. 

```{r appendix-I, out.width="100%", fig.cap="(ref:appendixicaption)", warning = FALSE}
knitr::include_graphics(paste("appendix","Appendix_I.png", sep =
                                .Platform$file.sep))
```


\newpage

# BOX A: Non-directional testing: best practice recommendations {#BOX-A}

* **When should we use the non-directional approach?**
    + Not all hypotheses are suitable for examination under the non-directional approach. Since the non-directional approach is targeted at detecting the presence of effects rather than their direction, it cannot be used to establish average differences between conditions at the group level (e.g., when comparing memory performance for items presented first and later in an experiment, rejecting a non-directional hypothesis does not entail evidence for an overall primacy or recency effect on recollection). In the case of unconscious processing, the theoretical question is regarding the presence or absence of a difference between the two conditions at the single-participant level, and as such, it lends itself to non-directional testing. Thus, selecting whether to use the non-directional or directional approach is directly linked to the theoretical question at stake.
    + A general recommendation is to plot the data of individual participants  with a measure of within-participant variability across trials (e.g., within-participant CI). As can be seen in Figure 1, the individual-level confidence intervals may guide researchers regarding the plausibility of non-significant group-level effects resulting from population variability in effect signs.
* **Which test should be used?**
    + When testing whether the experimental manipulation affects the dependent variable (e.g., either main or interaction effects on reaction times, accuracy, brain activity etc.), unless normality and equal variance of within-participant variability can be assumed with high certainty, we recommend using the absolute effect size test due to is computational efficiency and superior sensitivity.
    + When these assumptions hold, QUID or OANOVA can be used for effects that are measured on a trial-by-trial basis (as opposed to effects measured by summarizing data from multiple trials, e.g., d’ or correlation effects). Specifically, when prior data is available, we advise incorporating it into the analysis using QUID, and when examining an interaction effect, OANOVA provides an easy-to-use solution.
    + To test for the prevalence of individual-level effects, rather than the mere existence of an effect at the group level, we recommend using the prevalence approach [@ince2021a; @ince2022a]. More specifically, we recommend using GNT [@donhauser2018a] to test whether the data provide evidence for the presence of an effect for at least a single individual.
* **Non-directional tests require within-participant counterbalancing of confounding variables**
    + As we discuss in the text, special care should be given to counterbalancing of confounding variables when using the non-directional approach. Specifically, unlike standard directional tests, the effects of confounders are not averaged out at the group level when counterbalanced across participants. Thus, when designing an experiment to reveal non-directional effects, counterbalancing should be done not only across participants but also across trials within participants. It should be noted however that within-subject counterbalancing can result in a more confusing task from the participants’ point of view, requiring researchers to balance design simplicity with inferential scope.
* **How to interpret non-directional effects?**
    + In contrast to directional tests, where signal is measured relative to variability across individuals, in non-directional tests it is measured relative to variability across different trials, within an individual. Hence, a positive result of a directional test indicates that effects are consistent between participants, while a non-directional test reveals the presence of an effect on the dependent variable within participants, regardless of the alignment of within participants effects across participants.
    + A significant non-directional effect without a corresponding directional effect suggests reliable variability in effect signs across individuals. Whether this variability reflects transient or stable individual differences can be further tested by correlating individual effect scores from two experimental sessions: stable differences should result in a positive correlation. Whenever stable individual differences are observed, further research may be needed to identify the relevant personal traits that interact with the experimental manipulation. 
